{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 픽셀 노말라이제이션\n",
    "* Progressive GAN 에서 사용한 pixel normalization 레이어를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers, losses, metrics\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 10\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2**5\n",
    "EPOCHS = 10\n",
    "print(BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train/np.max(X_train), dtype=np.float32)\n",
    "X_test = np.array(X_test/np.max(X_test), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n",
      "(10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).shuffle(len(X_train))\n",
    "testdataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 픽셀 노말라이제이션\n",
    "* 알렉스 넷(Krizhevsky et al., 2012)에서 제안한 local response normalization을 아래 수식으로 변형하여 사용하고 pixel normalization 이라 이름 붙임\n",
    "\n",
    "$$\n",
    "b_{x,y} = {a_{x, y} \\over \\sqrt{{1\\over N} \\sum_{j=0}^{N-1} \\bigg( a^j_{x,y} \\bigg)^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "* 이 때, $\\epsilon=10^-8$\n",
    "* $N$ : feature map 개수\n",
    "* $a_{x,y}, b_{x,y}$ : 픽셀 $x,y$에 대한 원래 피처벡터와 노말라이즈드 된 피처벡터\n",
    "* 참고로 알렉스넷의 local respose normalizaiton 은 ```tf.nn.local_response_normalization```가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormalization(layers.Layer):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(PixelNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        return x * tf.math.rsqrt(tf.reduce_mean(tf.square(x), axis=None, keepdims=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얕은 모델에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(models.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = models.Sequential([\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.model(inputs, training=training)\n",
    "        return outputs\n",
    "    \n",
    "model = Model()\n",
    "model(X_train[:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sce = losses.SparseCategoricalCrossentropy()\n",
    "adam = optimizers.Adam(1e-3)\n",
    "loss = metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "    _x, _y = inputs\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        y_pred = model(_x, True)\n",
    "        _loss = sce(_y, y_pred)\n",
    "        \n",
    "    grads = t.gradient(_loss, model.trainable_variables)\n",
    "    adam.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "    loss.update_state(_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\tloss = 1.43167722\t32.09sec/epoch\n",
      "2/10\tloss = 0.99789804\t31.97sec/epoch\n",
      "3/10\tloss = 0.81246853\t31.60sec/epoch\n",
      "4/10\tloss = 0.70031887\t31.34sec/epoch\n",
      "5/10\tloss = 0.61313117\t31.92sec/epoch\n",
      "6/10\tloss = 0.53898168\t31.41sec/epoch\n",
      "7/10\tloss = 0.47794420\t31.75sec/epoch\n",
      "8/10\tloss = 0.41353273\t31.61sec/epoch\n",
      "9/10\tloss = 0.36280429\t31.26sec/epoch\n",
      "10/10\tloss = 0.31180003\t31.09sec/epoch\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    for inputs in traindataset:\n",
    "        train_step(inputs)\n",
    "        \n",
    "    print(f\"{e+1}/{EPOCHS}\\tloss = {loss.result():.8f}\\t{time.time()-start:.2f}sec/epoch\")\n",
    "    loss.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PN 없이 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74453866\n"
     ]
    }
   ],
   "source": [
    "sca = metrics.SparseCategoricalAccuracy()\n",
    "acc = metrics.Mean()\n",
    "\n",
    "for inputs in testdataset:\n",
    "    _x, _y = inputs\n",
    "    pred = model(_x)\n",
    "    acc.update_state(sca(_y, pred))\n",
    "    \n",
    "print(acc.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PN 넣은 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PN_Model(models.Model):\n",
    "    def __init__(self):\n",
    "        super(PN_Model, self).__init__()\n",
    "        self.model = models.Sequential([\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            PixelNormalization(),\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            PixelNormalization(),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            PixelNormalization(),\n",
    "            layers.Conv2D(64, (3,3), (1,1), activation=tf.nn.relu),\n",
    "            PixelNormalization(),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.model(inputs, training=training)\n",
    "        return outputs\n",
    "    \n",
    "pnmodel = PN_Model()\n",
    "pnmodel(X_train[:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pntrain_step(inputs):\n",
    "    _x, _y = inputs\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        y_pred = pnmodel(_x, True)\n",
    "        _loss = sce(_y, y_pred)\n",
    "        \n",
    "    grads = t.gradient(_loss, pnmodel.trainable_variables)\n",
    "    adam.apply_gradients(list(zip(grads, pnmodel.trainable_variables)))\n",
    "    loss.update_state(_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\tloss = 1.89180040\t49.31sec/epoch\n",
      "2/10\tloss = 1.50700188\t49.61sec/epoch\n",
      "3/10\tloss = 1.38040340\t49.14sec/epoch\n",
      "4/10\tloss = 1.28476465\t48.94sec/epoch\n",
      "5/10\tloss = 1.19977808\t49.37sec/epoch\n",
      "6/10\tloss = 1.12631416\t62.68sec/epoch\n",
      "7/10\tloss = 1.06056011\t48.67sec/epoch\n",
      "8/10\tloss = 1.00555885\t49.92sec/epoch\n",
      "9/10\tloss = 0.94893080\t48.11sec/epoch\n",
      "10/10\tloss = 0.90535581\t50.21sec/epoch\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    for inputs in traindataset:\n",
    "        pntrain_step(inputs)\n",
    "        \n",
    "    print(f\"{e+1}/{EPOCHS}\\tloss = {loss.result():.8f}\\t{time.time()-start:.2f}sec/epoch\")\n",
    "    loss.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PN 넣고 성능확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65336835\n"
     ]
    }
   ],
   "source": [
    "sca = metrics.SparseCategoricalAccuracy()\n",
    "acc = metrics.Mean()\n",
    "\n",
    "for inputs in testdataset:\n",
    "    _x, _y = inputs\n",
    "    pred = pnmodel(_x)\n",
    "    acc.update_state(sca(_y, pred))\n",
    "    \n",
    "print(acc.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 얌전히 GAN에나 쓰자."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
