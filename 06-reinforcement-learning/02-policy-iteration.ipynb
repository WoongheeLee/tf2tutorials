{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = env.observation_space.n\n",
    "A = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤하게 움직여서 겜 한판 돌려보자!\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(\"랜덤하게 움직여서 겜 한판 돌려보자!\")\n",
    "\n",
    "s = env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "while True:\n",
    "    s, r, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = .9\n",
    "EPISODES = 1000\n",
    "THETA = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init policy, S x A x 1\n",
    "pi = np.ones((S, A))\n",
    "pi /= A\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init rewrads, S x A x 1\n",
    "rewards = np.zeros((S, A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init transition matrix, A x S x S' x 1\n",
    "Pss = np.zeros((A, S, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init value function, S x 1\n",
    "valuefunction = np.ones((S)) / S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list\n",
    "return_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input $\\pi$, the policy to be evaluated</br>\n",
    "Algorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation</br>\n",
    "Initialize $V(s)$, for all $s\\in S^+$, arbitrarily except that $V (terminal) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop:</br>\n",
    "&nbsp;&nbsp;&nbsp;$\\Delta\\leftarrow 0$</br>\n",
    "&nbsp;&nbsp;&nbsp;Loop for each $s\\in S$:</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v\\leftarrow V(s)$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s)\\leftarrow \\sum_a \\pi (a|s) \\sum_{s',r}p(s', r|s, a)[r+\\gamma V(s')]$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\Delta \\leftarrow \\max (\\Delta, |v-V(s)|)$</br>\n",
    "until $\\Delta < \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.arange(A)\n",
    "\n",
    "def get_action(state):\n",
    "    return np.random.choice(action, size=None, p=pi[state])\n",
    "\n",
    "def get_transition(episodes):\n",
    "    global rewards, return_list\n",
    "    \n",
    "    _rewards = np.zeros(rewards.shape)\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = get_action(state)\n",
    "            state_, _reward, done, _ = env.step(action)\n",
    "            \n",
    "            Pss[action][state][state_] += 1 # cumulate again and again\n",
    "            _rewards[state][action] += _reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = state_\n",
    "                \n",
    "    # average rewards\n",
    "    _rewards /= episodes\n",
    "    rewards = _rewards\n",
    "    return_list.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLICY EVALUATION\n",
    "\n",
    "# 첨엔 100번\n",
    "get_transition(100)\n",
    "\n",
    "while True:\n",
    "    delta = .0\n",
    "\n",
    "    PssV = np.zeros((A, S))\n",
    "    pss_sum = np.sum(Pss)\n",
    "    for a in range(A):\n",
    "        for s in range(S):\n",
    "            _sum = 0.\n",
    "            for s_ in range(S):\n",
    "                _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "            PssV[a][s] = _sum\n",
    "            \n",
    "    PssV *= GAMMA\n",
    "    \n",
    "    Rsum = np.zeros((S, A))\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "    \n",
    "    newV = np.zeros((S))\n",
    "    for s in range(S):\n",
    "        _sum = 0.\n",
    "        for a in range(A):\n",
    "            newV[s] += pi[s][a] * Rsum[s][a]\n",
    "            \n",
    "    delta = max(delta, np.sum(np.abs(valuefunction - newV)))\n",
    "    \n",
    "    if delta < THETA:\n",
    "        break\n",
    "        \n",
    "    valuefunction = np.array(newV)\n",
    "    \n",
    "    # 그 담번엔 20번씩\n",
    "    get_transition(EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.91149035e-15, 1.28104729e-13, 2.22833996e-11, 2.90979244e-14,\n",
       "       1.64030633e-13, 0.00000000e+00, 2.27063363e-09, 0.00000000e+00,\n",
       "       2.92895437e-11, 3.66863060e-09, 4.72783949e-06, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.66060827e-06, 2.25294061e-03, 0.00000000e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valuefunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.003, 0.003, 0.003],\n",
       "       [0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$policy-stable \\leftarrow true$</br>\n",
    "For each $s\\in S$:</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$old-action \\leftarrow \\pi(s)$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s',r} p(s', r | s, a)[r + \\gamma V(s')]$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;If $old-action \\neq \\pi(s)$, then $policy-stable\\leftarrow false$</br>\n",
    "If $policy-stable$, then stop and return $V\\approx v_*$ and $]pi \\approx \\pi_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLICY IMPROVEMENT\n",
    "\n",
    "\n",
    "old_action = np.array(pi)\n",
    "\n",
    "PssV = np.zeros((A, S))\n",
    "pss_sum = np.sum(Pss)\n",
    "for a in range(A):\n",
    "    for s in range(S):\n",
    "        _sum = 0.\n",
    "        for s_ in range(S):\n",
    "            _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "        PssV[a][s] = _sum\n",
    "\n",
    "PssV *= GAMMA\n",
    "\n",
    "new_action = np.zeros((S, A))\n",
    "for s in range(S):\n",
    "    for a in range(A):\n",
    "        Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "for s in range(S):\n",
    "    if np.min(Rsum[s]) != np.max(Rsum[s]):\n",
    "        new_action[s][np.argmax(Rsum[s])] = 1.\n",
    "    else:\n",
    "        new_action[s] = pi[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have pi* and v*!\n",
      "pi*\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "v*\n",
      "[2.21057395e-10 2.98031834e-09 9.13250289e-08 1.17547844e-10\n",
      " 1.16523952e-09 0.00000000e+00 3.24644594e-06 0.00000000e+00\n",
      " 4.94036832e-08 2.61740804e-06 4.22549120e-04 0.00000000e+00\n",
      " 0.00000000e+00 1.41873138e-04 6.62734924e-02 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# 첨엔 100번\n",
    "get_transition(100)\n",
    "\n",
    "while True:\n",
    "\n",
    "    '''\n",
    "    POLICY EVALUATION\n",
    "    '''\n",
    "\n",
    "    while True:\n",
    "        delta = .0\n",
    "\n",
    "        PssV = np.zeros((A, S))\n",
    "        pss_sum = np.sum(Pss)\n",
    "        for a in range(A):\n",
    "            for s in range(S):\n",
    "                _sum = 0.\n",
    "                for s_ in range(S):\n",
    "                    _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "                PssV[a][s] = _sum\n",
    "\n",
    "        PssV *= GAMMA\n",
    "\n",
    "        Rsum = np.zeros((S, A))\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "\n",
    "        newV = np.zeros((S))\n",
    "        for s in range(S):\n",
    "            _sum = 0.\n",
    "            for a in range(A):\n",
    "                newV[s] += pi[s][a] * Rsum[s][a]\n",
    "\n",
    "        delta = max(delta, np.sum(np.abs(valuefunction - newV)))\n",
    "\n",
    "        if delta < THETA:\n",
    "            break\n",
    "\n",
    "        valuefunction = np.array(newV)\n",
    "\n",
    "        get_transition(EPISODES)\n",
    "        \n",
    "        return_list.append(np.sum(rewards))\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    POLICY IMPROVEMENT\n",
    "    \"\"\"\n",
    "\n",
    "    policy_stable = True\n",
    "\n",
    "    old_action = np.array(pi)\n",
    "\n",
    "    PssV = np.zeros((A, S))\n",
    "    pss_sum = np.sum(Pss)\n",
    "    for a in range(A):\n",
    "        for s in range(S):\n",
    "            _sum = 0.\n",
    "            for s_ in range(S):\n",
    "                _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "            PssV[a][s] = _sum\n",
    "\n",
    "    PssV *= GAMMA\n",
    "\n",
    "    new_action = np.zeros((S, A))\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "    for s in range(S):\n",
    "        if np.min(Rsum[s]) != np.max(Rsum[s]):\n",
    "            new_action[s][np.argmax(Rsum[s])] = 1.\n",
    "        else:\n",
    "            new_action[s] = pi[s]\n",
    "\n",
    "\n",
    "    if not (old_action == new_action).all():\n",
    "        policy_stable = False\n",
    "        pi = np.array(new_action)\n",
    "        \n",
    "    if policy_stable:\n",
    "        print(\"We have pi* and v*!\")\n",
    "        print(\"pi*\")\n",
    "        print(pi)\n",
    "        print(\"v*\")\n",
    "        print(valuefunction)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in range(A):\n",
    "#     for s in range(S):\n",
    "#         for s_ in range(S):\n",
    "#             print(f\"(a:{a}, s:{s}) -> s':{s_} : {Pss[a][s][s_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration 으로 풀었을 때, 에이전트 퍼포먼스: 1000회 중 67회 성공 -> 6.70%\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "cnt_success = 0\n",
    "trial = 1000\n",
    "\n",
    "for _ in range(trial):\n",
    "    while True:\n",
    "        s, r, done, _ = env.step(get_action(s))\n",
    "\n",
    "        cnt_success += r\n",
    "\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            break\n",
    "        \n",
    "print(f\"policy iteration 으로 풀었을 때, 에이전트 퍼포먼스: {trial}회 중 {cnt_success:n}회 성공 -> {cnt_success/trial * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아무곳이나 가는 에이전트 퍼포먼스: 1000회 중 12회 성공 -> 1.20%\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "cnt_success = 0\n",
    "trial = 1000\n",
    "\n",
    "for _ in range(trial):\n",
    "    while True:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "        cnt_success += r\n",
    "\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            break\n",
    "        \n",
    "print(f\"아무곳이나 가는 에이전트 퍼포먼스: {trial}회 중 {cnt_success:n}회 성공 -> {cnt_success/trial * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
