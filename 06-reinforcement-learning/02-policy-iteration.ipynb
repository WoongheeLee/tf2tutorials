{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "* Dynamic programming 중에 하나\n",
    "* DP 는 MDP에 관한 모든 정보를 알고 있다고 가정\n",
    "* Prediction 문제 (value function 찾는 문제)\n",
    "  * 입력: MDP $<S, A, P, R, \\gamma>$ 와 policy $\\pi$\n",
    "  * 또는 MRP $<S, P^{\\pi}, R^{\\pi}, \\gamma>$\n",
    "  * 출력: value function $v_{\\pi}$\n",
    "* Control 문제 (policy 찾는 문제)\n",
    "  * 입력: MDP $<S, A, P, R, \\gamma>$\n",
    "  * 출력: optimal value function $v_*$\n",
    "  * 그리고 optimal policy $\\pi_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy evaluation + policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "* Frozen lake 환경을 세팅\n",
    "* Gym 에서 discrete 한 environment 쓸만한 놈이 이거라서 가져옴\n",
    "* 'S' 타일에서 시작\n",
    "* Action으로 'Up', 'Left', 'Right', 'Down'을 줄 수 있음\n",
    "* 에피소드는 'H' 타일에 빠져서 reward 0 (죽음) 얻거나, 'G' (goal) 타일로 가서 reward 1 (점수) 얻거나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```is_slippery``` 를 True, False에 따라\n",
    "* True: action에 따라 확정적으로 이동하지 않고, 확률적으로 action 말고 다른데로 가버림 (frozen lake라 미끄러워서 그렇다고 함...)\n",
    "* False: action 에 따라 확정적으로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "action space: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "S = env.observation_space.n\n",
    "A = env.action_space.n\n",
    "print('state space:', np.arange(S))\n",
    "print('action space:', np.arange(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤하게 움직여서 겜 한판 돌려보자!\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(\"랜덤하게 움직여서 겜 한판 돌려보자!\")\n",
    "s = env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "while True:\n",
    "    s, r, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constant\n",
    "* $\\gamma$ : discount factor\n",
    "* $\\theta$ : policy evaluation threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = .9 # discount factor\n",
    "EPISODES = 1000 # policy evaluation 끝나고 transition matrix 업데이트 할 횟수\n",
    "THETA = 1e-4 # policy evaluation threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\pi$ : policy. state에 따라 어떤 action 취할지 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init policy, S x A x 1\n",
    "pi = np.ones((S, A))\n",
    "pi /= A\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 리워드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init rewrads, S x A x 1\n",
    "rewards = np.zeros((S, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transition (model 이라고도 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init transition matrix, A x S x S' x 1\n",
    "Pss = np.zeros((A, S, S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* value function : 이 예제에서는 state-value function 을 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init value function, S x 1\n",
    "valuefunction = np.ones((S)) / S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list\n",
    "return_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy evaluation\n",
    "* prediciton 문제\n",
    "* 어떤 policy 가 주어졌다 치고, value function 찾기\n",
    "* Solution: Bellman 기대식을 iterative 하게 적용\n",
    "* synchronous 로 업뎃\n",
    "  * 각 iteration $k+1$\n",
    "  * 모든 states $s\\in S$에 대해서\n",
    "  * $v_k(s')$ 로 부터 $v_{k+1}(s)$를 업뎃\n",
    "  * 이 때 $s'$는 $s$로 부터 뒤따라 나온 state\n",
    "\n",
    "(asynchronous 방법은 나중에...)\n",
    "\n",
    "($v_{\\pi}$ 의 convergence 증명은 Sutton 책에...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bellman expectation equation\n",
    "\n",
    "\\begin{align}\n",
    "v_{k+1}(s) & = \\sum_{a\\in A} \\pi (a|s) \\big( R^a_s + \\gamma \\sum_{s'\\in S} P^a_{ss'} v_k (s') \\big)\\\\\n",
    "v^{k+1} & = R^{\\pi} + \\gamma P^{\\pi} v^k\n",
    "\\end{align}\n",
    "\n",
    "* 그냥 별 쓰잘데기 없는 이상한 policy를 써서라도 v를 업뎃함\n",
    "* k가 k+1 로 진행됨에 따라 reward는 정확하기 때문에 v가 올바른 값이 얻어지게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy evaluation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input $\\pi$, the policy to be evaluated</br>\n",
    "Algorithm parameter: a small threshold $\\theta > 0$ determining accuracy of estimation</br>\n",
    "Initialize $V(s)$, for all $s\\in S^+$, arbitrarily except that $V (terminal) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop:</br>\n",
    "&nbsp;&nbsp;&nbsp;$\\Delta\\leftarrow 0$</br>\n",
    "&nbsp;&nbsp;&nbsp;Loop for each $s\\in S$:</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v\\leftarrow V(s)$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s)\\leftarrow \\sum_a \\pi (a|s) \\sum_{s',r}p(s', r|s, a)[r+\\gamma V(s')]$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\Delta \\leftarrow \\max (\\Delta, |v-V(s)|)$</br>\n",
    "until $\\Delta < \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래 구현에서는 $r$에 $p(s',r|s,a)$ 곱하지 않고 그냥 $r$ 씀\n",
    "* 즉 아래 식으로 업뎃\n",
    "$$\n",
    "v^{k+1} = R^\\pi + \\gamma P^\\pi v^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.arange(A)\n",
    "\n",
    "def get_action(state):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        state: 상태\n",
    "    return: \n",
    "        policy 의 확률에 따라 취할 수 있는 action 중에서 하나 구해서 리턴\n",
    "    \"\"\"\n",
    "    return np.random.choice(action, size=None, p=pi[state])\n",
    "\n",
    "def get_transition(episodes):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        episodes 수\n",
    "    return: \n",
    "        None\n",
    "        에피소드 수 주어진 만큼 environment 돌아다니면서 transition matrix 채움\n",
    "    \"\"\"\n",
    "    global rewards, return_list\n",
    "    \n",
    "    _rewards = np.zeros(rewards.shape)\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = get_action(state)\n",
    "            state_, _reward, done, _ = env.step(action)\n",
    "            \n",
    "            Pss[action][state][state_] += 1 # cumulate again and again\n",
    "            _rewards[state][action] += _reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = state_\n",
    "                \n",
    "    # average rewards\n",
    "    _rewards /= episodes\n",
    "    rewards = _rewards\n",
    "    return_list.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLICY EVALUATION\n",
    "\n",
    "# 에피소드 수 만큼 돌면서 transition matrix를 채움\n",
    "get_transition(episodes=100)\n",
    "\n",
    "while True:\n",
    "    # loop 멈출 현재의 threshold\n",
    "    delta = .0\n",
    "\n",
    "    PssV = np.zeros((A, S))\n",
    "    pss_sum = np.sum(Pss)\n",
    "    for a in range(A):\n",
    "        for s in range(S):\n",
    "            _sum = 0.\n",
    "            for s_ in range(S):\n",
    "                _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "            PssV[a][s] = _sum\n",
    "            \n",
    "    PssV *= GAMMA\n",
    "    \n",
    "    Rsum = np.zeros((S, A))\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "    \n",
    "    newV = np.zeros((S))\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            newV[s] += pi[s][a] * Rsum[s][a]\n",
    "            \n",
    "    delta = max(delta, np.sum(np.abs(valuefunction - newV)))\n",
    "    \n",
    "    if delta < THETA:\n",
    "        break\n",
    "        \n",
    "    valuefunction = np.array(newV)\n",
    "    \n",
    "    # 그 담번엔 EPISODES번씩\n",
    "    get_transition(EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.47957265e-15, 9.22095298e-14, 7.95191468e-12, 2.11385951e-14,\n",
       "       1.18045103e-13, 0.00000000e+00, 2.45071906e-09, 0.00000000e+00,\n",
       "       1.05244224e-11, 4.08203959e-09, 2.69967727e-06, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.12303247e-06, 2.75156677e-03, 0.00000000e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valuefunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.011, 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy improvement\n",
    "* control 문제\n",
    "* value function 이 optimal 이라 치고 policy 찾기 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Policy $\\pi$가 주어졌을 때\n",
    "  * policy $\\pi$ 를 **evaluate**\n",
    "  $$\n",
    "  v_{\\pi}(s) = \\mathbb{E} \\big[ R_{t+1} + \\gamma R_{t+2} + \\cdots | S_t = s \\big]\n",
    "  $$\n",
    "  * $v_{\\pi}$ 에 따라서 policy 를 greedy 하게 **improve**\n",
    "  $$\n",
    "  \\pi' = \\mbox{greedy}(v_{\\pi})\n",
    "  $$\n",
    "  \n",
    "* 작은 그리드 위에서 goal 찾기 (예: frozen lake에서 action 이 random 아니면) 는 걍 improved policy 한 번 하면 바로 optimal policy $\\pi^*$가 찾아짐\n",
    "* 근데 일반적으로는 improvement / evaluation 을 몇 iteration 돌려주어야 함\n",
    "* **Policy iteration** 이 항상 $\\pi^*$에 converge 함\n",
    "* 즉 되도 않은 value function 이랑 되도 않은 policy 로 시작해도 policy iteration 하면 optimal policy 찾아진다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy improvement 증명\n",
    "(improvement 로 진짜 더 나은 policy 찾을 수 있음?)\n",
    "* deterministic policy $a=\\pi (s)$ 에 대해서\n",
    "* greedy 하게 policy 를 *improve* 가능\n",
    "$$\n",
    "\\pi' (s) = \\arg \\max_{a\\in A} q_{\\pi}(s, a)\n",
    "$$\n",
    "* 이를 통해 한 스텝으로 어떤 state $s$ 로부터 value 를 improve 가능\n",
    "$$\n",
    "q_{\\pi} (s, \\pi'(s)) = \\max_{a\\in A} q_{\\pi} (s, a) \\ge q_{\\pi} (s, \\pi (s)) = v_{\\pi} (s)\n",
    "$$\n",
    "* 그래서 value function 을 improve 함, $v_{\\pi'} (s) \\ge v_{\\pi} (s)$\n",
    "\\begin{align}\n",
    "v_{\\pi} (s) & \\le q_{\\pi}(s, \\pi'(s)) = \\mathbb{E}_{\\pi'} [R_{t+1} + \\gamma v_{\\pi} (S_{t+1}) | S_t = s]\\\\\n",
    "& \\le \\mathbb{E}_{\\pi'} [R_{t+1} + \\gamma q_{\\pi}(S_{t+1},\\pi'(S_{t+1}))|S_t = s]\\\\\n",
    "& \\le \\mathbb{E}_{\\pi'} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_{\\pi}(S_{t+2}, \\pi'(S_{t+2}))|S_t = s]\\\\\n",
    "& \\le \\mathbb{E}_{\\pi'} [R_[t+1] + \\gamma R_{t+2} + \\cdots | S_t = s] = v_{\\pi'}(s)\n",
    "\\end{align}\n",
    "* 만약에 improvement 가 멈추면\n",
    "$$\n",
    "q_{\\pi} (s, \\pi'(s)) = \\max_{a\\in A} q_{\\pi} (s, a) = q_{\\pi} (s, \\pi (s)) = v_{\\pi} (s)\n",
    "$$\n",
    "* 그러면 Bellman 최적등식이 만족\n",
    "$$\n",
    "v_{\\pi} (s) = \\max_{a\\in A} q_{\\pi} (s, a)\n",
    "$$\n",
    "* 그래서 모든 $s\\in S$ 에 대해 $v_{\\pi}(s) = v_* (s)$\n",
    "* 그래서 $\\pi$는 optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy improvement algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$policy-stable \\leftarrow true$</br>\n",
    "For each $s\\in S$:</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$old-action \\leftarrow \\pi(s)$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s',r} p(s', r | s, a)[r + \\gamma V(s')]$</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;If $old-action \\neq \\pi(s)$, then $policy-stable\\leftarrow false$</br>\n",
    "If $policy-stable$, then stop and return $V\\approx v_*$ and $]pi \\approx \\pi_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLICY IMPROVEMENT\n",
    "\n",
    "# old-action 과 new-action 비교용 임시저장공간. 이 셀에서는 사용X\n",
    "old_action = np.array(pi)\n",
    "\n",
    "PssV = np.zeros((A, S))\n",
    "\n",
    "# Transition matrix 에 count 저장된거 이걸로 나눠주기 해서 확률로 바꾸는 작업\n",
    "pss_sum = np.sum(Pss) # 실은 axis=-1로 해주어야 함\n",
    "for a in range(A):\n",
    "    for s in range(S):\n",
    "        _sum = 0.\n",
    "        for s_ in range(S):\n",
    "            _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "        PssV[a][s] = _sum\n",
    "\n",
    "# 위 수식에서 감마 곱하기 까지 작업\n",
    "PssV *= GAMMA\n",
    "\n",
    "# 리워드도 더해주는 작업, 저 위에 알고리즘과 약간 다른 수식 사용함\n",
    "for s in range(S):\n",
    "    for a in range(A):\n",
    "        Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "        \n",
    "# state별로 max value (greedy) 인 action 취하도록 policy 수정\n",
    "new_action = np.zeros((S, A))\n",
    "for s in range(S):\n",
    "    if np.min(Rsum[s]) != np.max(Rsum[s]):\n",
    "        new_action[s][np.argmax(Rsum[s])] = 1.\n",
    "    else:\n",
    "        new_action[s] = pi[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have pi* and v*!\n",
      "pi*\n",
      "[[0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "v*\n",
      "[3.33897947e-05 2.57076660e-09 3.62931964e-07 5.97495331e-10\n",
      " 2.39477736e-04 0.00000000e+00 1.12459477e-04 0.00000000e+00\n",
      " 1.86470601e-03 1.50064188e-02 1.22379743e-01 0.00000000e+00\n",
      " 0.00000000e+00 3.88706446e-04 1.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# 첨엔 100번\n",
    "get_transition(100)\n",
    "\n",
    "while True:\n",
    "\n",
    "    '''\n",
    "    POLICY EVALUATION\n",
    "    '''\n",
    "\n",
    "    while True:\n",
    "        delta = .0\n",
    "\n",
    "        PssV = np.zeros((A, S))\n",
    "        pss_sum = np.sum(Pss)\n",
    "        for a in range(A):\n",
    "            for s in range(S):\n",
    "                _sum = 0.\n",
    "                for s_ in range(S):\n",
    "                    _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "                PssV[a][s] = _sum\n",
    "\n",
    "        PssV *= GAMMA\n",
    "\n",
    "        Rsum = np.zeros((S, A))\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "\n",
    "        newV = np.zeros((S))\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                newV[s] += pi[s][a] * Rsum[s][a]\n",
    "\n",
    "        delta = max(delta, np.sum(np.abs(valuefunction - newV)))\n",
    "\n",
    "        if delta < THETA:\n",
    "            break\n",
    "\n",
    "        valuefunction = np.array(newV)\n",
    "\n",
    "        get_transition(EPISODES)\n",
    "        \n",
    "        return_list.append(np.sum(rewards))\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    POLICY IMPROVEMENT\n",
    "    \"\"\"\n",
    "\n",
    "    policy_stable = True\n",
    "\n",
    "    old_action = np.array(pi)\n",
    "\n",
    "    PssV = np.zeros((A, S))\n",
    "    pss_sum = np.sum(Pss)\n",
    "    for a in range(A):\n",
    "        for s in range(S):\n",
    "            _sum = 0.\n",
    "            for s_ in range(S):\n",
    "                _sum += Pss[a][s][s_]/pss_sum * valuefunction[s_]\n",
    "            PssV[a][s] = _sum\n",
    "\n",
    "    PssV *= GAMMA\n",
    "\n",
    "    new_action = np.zeros((S, A))\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            Rsum[s][a] += rewards[s][a] + PssV[a][s]\n",
    "    for s in range(S):\n",
    "        if np.min(Rsum[s]) != np.max(Rsum[s]):\n",
    "            new_action[s][np.argmax(Rsum[s])] = 1.\n",
    "        else:\n",
    "            new_action[s] = pi[s]\n",
    "\n",
    "\n",
    "    if not (old_action == new_action).all():\n",
    "        policy_stable = False\n",
    "        pi = np.array(new_action)\n",
    "        \n",
    "    if policy_stable:\n",
    "        print(\"We have pi* and v*!\")\n",
    "        print(\"pi*\")\n",
    "        print(pi)\n",
    "        print(\"v*\")\n",
    "        print(valuefunction)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration으로 찾은 policy에 따라 겜 해보자!\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy iteration으로 찾은 policy에 따라 겜 해보자!\")\n",
    "s = env.reset()\n",
    "env.render()\n",
    "\n",
    "while True:\n",
    "    s, r, done, _ = env.step(get_action(s))\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print('reward:',r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in range(A):\n",
    "#     for s in range(S):\n",
    "#         for s_ in range(S):\n",
    "#             print(f\"(a:{a}, s:{s}) -> s':{s_} : {Pss[a][s][s_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration 으로 풀었을 때, 에이전트 퍼포먼스: 1000회 중 1000회 성공 -> 100.00%\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "cnt_success = 0\n",
    "trial = 1000\n",
    "\n",
    "for _ in range(trial):\n",
    "    while True:\n",
    "        s, r, done, _ = env.step(get_action(s))\n",
    "        cnt_success += r\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            break\n",
    "        \n",
    "print(f\"policy iteration 으로 풀었을 때, 에이전트 퍼포먼스: {trial}회 중 {cnt_success:n}회 성공 -> {cnt_success/trial * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아무곳이나 가는 에이전트 퍼포먼스: 1000회 중 8회 성공 -> 0.80%\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "cnt_success = 0\n",
    "trial = 1000\n",
    "\n",
    "for _ in range(trial):\n",
    "    while True:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "        cnt_success += r\n",
    "\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            break\n",
    "        \n",
    "print(f\"아무곳이나 가는 에이전트 퍼포먼스: {trial}회 중 {cnt_success:n}회 성공 -> {cnt_success/trial * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
