{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Free Control\n",
    "* Control\n",
    "  * input: MDP\n",
    "  * output: optimal valuefunction and optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이전 튜토리얼 model-free prediction에서 MDP 모를 때 value function **추정**\n",
    "* 여기 model-free control에서는 MDP 모를 때 value function **최적화**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On and Off-Policy Learning\n",
    "* On-policy learning\n",
    "  * 자기 자신으로부터 배움\n",
    "  * policy $\\pi$를 $\\pi$의 경험으로부터 학습\n",
    "* Off-policy learning\n",
    "  * 어깨너머로 배움\n",
    "  * policy $\\pi$를 $\\mu$의 경험으로부터 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration 복습\n",
    "* Policy evaluation\n",
    "  * $v_\\pi$를 추정\n",
    "  * 예: iterative policy evaluation\n",
    "* Policy improvement\n",
    "  * $\\pi' \\ge \\pi$ 생성\n",
    "  * 예: greedy policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Evaluation 을 이용한 Generalized Policy Iteration\n",
    "* Policy evaluation\n",
    "  * Monte-Carlo policy evaluation, $V=v_\\pi$ 로 수행한다고 치고\n",
    "* Policy improvement\n",
    "  * Greedy policy improvement 하면 되지 않남?\n",
    "  * 되지 않아\n",
    "  * 왜냐면 V(s)를 greedy policy improvement 하려면 MDP 알아야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Policy Iteration Using Action-Value Function\n",
    "* V(s) 대신에 Q(s, a)는 model-free 임\n",
    "$$\n",
    "\\pi'(s) = \\arg \\max_{a\\in A} Q(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration with Action-Value Function\n",
    "* Policy evaluation\n",
    "  * Monte-Carlo policy evaluation, $Q = q_\\pi$\n",
    "* Policy improvement\n",
    "  * Greedy policy improvement 어떻게? 아래에서 알아보자.\n",
    "  * 요거 greedy 하게만 action 선택하면 잘못된 경험으로부터 이상한 행동에 stuck할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy Exploration\n",
    "* 모든 액션 m 개가 non-zero probability\n",
    "* **확률 $1-\\epsilon$ 으로 greedy action 하고**\n",
    "* **확률 $\\epsilon$ 으로 무작위 액션 취함**\n",
    "$$\n",
    "\\pi (a|s)=\n",
    "\\begin{cases}\n",
    "\\epsilon/m + 1 - \\epsilon & \\mbox{if }a^* = \\arg \\max_{a\\in A} Q(s, a)\\\\\n",
    "\\epsilon/m & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "* $\\epsilon$-greedy 로 점점 더 나은 policy 찾아지는 것 증명 되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Policy Iteration\n",
    "* Policy evaluation\n",
    "  * Monte-Carlo policy evaluation, $Q = q_\\pi$\n",
    "* Policy improvement\n",
    "  * $\\epsilon$-greedy policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Control\n",
    "* 요걸 여러 에피소드에 대해서 evaluation 하고, 그 다음에 improvement 적용할 수 있지만,\n",
    "* 그게 아니라 한 에피소드에 대해서 policy evaluation 하고, 곧 바로 policy improvement 할 수도 있음\n",
    "* Policy evaluation\n",
    "  * Monte-Carlo policy evaluation, $Q \\approx q_\\pi$\n",
    "* Policy improvement\n",
    "  * $\\epsilon$-greedy policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLIE\n",
    "* 정의: Greedy in the Limit with Infinite Exploration (GLIE)\n",
    " * 모든 state-action 쌍이 무한 번 탐색됨\n",
    " $$\n",
    " \\lim_{k\\rightarrow \\infty} N_k (s, a) = \\infty\n",
    " $$\n",
    " * greedy policy 에 따라 policy 가 수렴함\n",
    " $$\n",
    " \\lim_{k \\rightarrow \\infty} \\pi_k (a|s) = \\mathbf{1}(a = \\arg \\max_{a'\\in A}Q_k (s, a'))\n",
    " $$\n",
    " \n",
    "* 예를 들어 만약 $\\epsilon_k = {1\\over k}$와 같이 $\\epsilon$ 이 0 으로 감소하는 $\\epsilon$-greedy 는 GLIE임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLIE Monte-Carlo Control\n",
    "* $k$번째 에피소드에서 $\\pi$로 샘플링: $\\{ S_1, A_1, R_2, \\cdots, S_T \\} ~ \\pi$\n",
    "* 에피소드의 각각의 state $S_t$와 action $A_t$에 대해서 \n",
    "\\begin{align}\n",
    "N(S_t, A) & \\leftarrow N(S_t, A_t) + 1\\\\\n",
    "Q(S_t, A) & \\leftarrow Q(S_t, A_t) + {1 \\over N(S_t, A_t)} (G_t - Q(S_t, A_t))\n",
    "\\end{align}\n",
    "* policy를 새로운 action-value function 에 따라 improve\n",
    "\\begin{align}\n",
    "\\epsilon & \\leftarrow 1/k\\\\\n",
    "\\pi & \\leftarrow \\epsilon\\mbox{-greedy}(Q)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정리: GLIE Monte-Carlo control은 optimal action-value function 으로 수렴함 (증명은 알아서 찾아보기) 즉, $Q(s, a) \\rightarrow q_* (s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC vs. TD Control\n",
    "* TD 가 MC 보다 나은 점\n",
    "  * 낮은 variance\n",
    "  * Online\n",
    "  * 미완성 sequences 사용 가능\n",
    "* 자연스럽게 control 단계에서 MC 대신 TD를 쓰자.\n",
    "  * Q(S, A) 에 TD 적용\n",
    "  * $\\epsilon$-greedy policy improvement\n",
    "  * 매 time-step 마다 업뎃 (MC 는 epsiode 가 끝난 다음에 업뎃했다는 차이점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Action-Value Functions with Sarsa\n",
    "$$\n",
    "Q(S, A) \\leftarrow Q(S, A) + \\alpha \\big( R + \\gamma Q(S', A') - Q(S, A)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Policy Control With Sarsa\n",
    "* 매 time-step 마다 (episode 끝나는거 말고)\n",
    "  * Policy evaluation: Sarsa, $Q \\approx q_\\pi$\n",
    "  * Policy improvement: $\\epsilon$-greedy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa Algorithm for On-Policy Control\n",
    "알고리즘.... 구현....\n",
    "\n",
    "* optimal action-value function 에 수렴함이 증명되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-Step Sarsa\n",
    "정리....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward view 랑 back ward view 등도 정리...\n",
    "\n",
    "살사 람다 알고리즘 등도...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy Learning\n",
    "* behaviour policy $\\mu (a|s)$ 를 따라서\n",
    "* $v_\\pi (s)$ 또는 $q_\\pi (s, a)$ 를 계산하기 위해 target policy $\\pi (a|s)$ 를 evaluate 함\n",
    "* 이게 왜 중요?\n",
    "  * 다른 agents 로부터 배울 수 있음 (예: 알파고가 사람 기보로 부터 학습)\n",
    "  * 이전 방법들은 policy 업뎃 되면 이전 policy 했던 행동들 다 잊어버리지만 이 방법은 이전 방법도 재사용 가능\n",
    "  * 탐색만 하는 policy를 두고, optimal policy 를 학습시키는게 가능\n",
    "  * 한 policy를 여러 policy가 학습가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling\n",
    "* 다른 분포로부터 기대값을 추정하려면 아래와 같은 방법 사용 가능\n",
    "* 예를 들어 P를 따르는 분포로부터 Q 를 따르는 기대값을 알고 싶으면\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{X\\sim P}[f(X)] & = \\sum P(X) f(X)\\\\\n",
    "& = \\sum Q(X) {P(X) \\over Q(X)} f(X)\\\\\n",
    "& = \\mathbb{E}_{X\\sim Q} \\Big[{P(X) \\over Q(X)} f(X) \\Big]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling for Off-Policy Monte-Carlo\n",
    "* importance sampling 을 써서 $\\mu$ 로부터 $\\pi$ 의 return 을 얻어내려면\n",
    "$$\n",
    "G_t^{\\pi / \\mu} = {\\pi (A_t | S_t) \\pi (A_{t+1} | S_{t+1}\\over \\mu(A_t|S_t)\\mu(A_{t+1} | S_{t+1}} \\cdots {\\pi (A_T|S_T)\\over \\mu (A_T|S_T)} G_t\n",
    "$$\n",
    "* value 업데이트는 위 return을 대입\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^{\\pi / \\mu} - V(S_t))\n",
    "$$\n",
    "* 근데 이거 현실에서는 안씀\n",
    "* 왜냐면 $\\mu$나 $\\pi$ 분모랑 분자가 엄청 많이 곱해져서 0이 되버리거나 엄청 큰 수가 되어버릴 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling for Off-Policy TD\n",
    "* 근데 TD는 한 스텝만 해줌\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\Bigg( {\\pi(A_t|S_t) \\over \\mu(A_t|S_t)} (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\Bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "* action-values Q(s, a)에 대한 off-policy learning 생각해보기\n",
    "* importance sampling 안쓰고\n",
    "* 다음 action은 behaviour policy $A_{t+1} \\sim \\mu(\\cdot | S_t)$ 에서 뽑음\n",
    "* 그러면 $Q(S_{t+1}, A_{t+1})$ 이 TD target에 가야겠지만\n",
    "* 그런데 뒤따르는 대체 action $A'\\sim \\pi(\\cdot | S_t)$로 간주함\n",
    "* 즉 $Q(S_{t+1}, A_{t+1})$ 이 들어가야할 자리는 $\\pi$로 뽑아서 $Q(S_{t+1}, A')$를 TD target에 넣음\n",
    "* 그니깐 현재는 $\\mu$로 value 가져오고, 그 다음 스텝에 관한 state $S_{t+1}$ 에 대해서 action 만 $\\pi$로 뽑아서, 그 value를 TD target에 넣는다는 말\n",
    "* 아래와 같이 업뎃\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A') -Q(S_t, A_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy Control with Q-Learning\n",
    "* behaviour $\\mu$랑 target policy $\\pi$ 모두 improve 하기로 해보자.\n",
    "* target policy $\\pi$ 는 Q(s, a)에 대해서 greedy로 정함\n",
    "$$\n",
    "\\pi (S_{t+1}) = \\arg \\max_{a'} Q(S_{t+1}, a')\n",
    "$$\n",
    "* Behaviour policy $\\mu$ 는 Q에 관해 $\\epsilon$-greedy 로 정함($\\mu$의 현재 상태에서 최선의 선택도 하고, $\\epsilon$확률로 랜덤한 행동도 하겠다는 것)\n",
    "* Q-learning target 은 간소화 시킬 수 있음\n",
    "\\begin{align}\n",
    "& R_{t+1} + \\gamma Q(S_{t+1}, A')\\\\\n",
    "= & R_{t+1} + \\gamma Q(S_{t+1}, \\arg \\max_{a'} Q(S_{t+1}, a'))\\\\\n",
    "= & R_{t+1} + \\max_{a'} \\gamma Q(S_{t+1}, a')\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Control Algorithm\n",
    "$$\n",
    "Q(S, A) \\leftarrow Q(S, A) + \\alpha (R + \\gamma \\max_{a'} Q(S', a') - Q(S,A))\n",
    "$$\n",
    "* Q-learning control 은 optimal action-value function 에 수렴함 증명되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm for Off-Policy Control\n",
    "알고리즘 정리, 구현...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Between DP and TD\n",
    "|Full Backup (DP)|Sample Backup (TD)|\n",
    "|---------|---------|\n",
    "|Iterative Policy Evaluation<br>$V(s)\\leftarrow \\mathbb{E}[R+\\gamma V(S')|s]$|TD Learning<br>$V(S)\\leftarrow^\\alpha R+\\gamma V(S')$|\n",
    "|Q-Policy Iteration<br>$Q(s,a)\\leftarrow \\mathbb{E}[R+\\gamma Q(S',A')|s,a]$|Sarsa<br>$Q(S,A)\\leftarrow^\\alpha R+\\gamma Q(S', A')$|\n",
    "|Q-Value Iteration<br>$Q(s,a)\\leftarrow \\mathbb{E}[R+\\gamma \\max_{a'\\in A}]$|Q-Learning<br>$Q(S,A)\\leftarrow^\\alpha R + \\gamma \\max_{a'\\in A} Q(S',a')$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
