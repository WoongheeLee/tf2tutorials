{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-Level Control Through Deep Reinforcement Learning\n",
    "* DQN\n",
    "* nature 2015\n",
    "* 구글 딥마인드 연구\n",
    "* DQN 논문 리뷰 영상 https://www.youtube.com/watch?v=eJXQKEtPvhY 의 슬라이드 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "* input: 게임화면을 state로 주고, 게임 점수는 reward로 줌\n",
    "* output: reward 기대값 최대가 되는 policy 찾기\n",
    "* 구체적인 state를 주지않고, 게임 pixel만 줘서 사람보다 게임 잘하는 agent 를 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사전 지식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "* TD target은 우리가 학습시키려는 policy $\\pi$에서 greedy 하게 뽑고\n",
    "\n",
    "$$\n",
    "\\pi (S_{t+1}) = \\arg \\max_{a'} Q(S_{t+1}, a')\n",
    "$$\n",
    "\n",
    "* 현재 value는 behaviour policy $\\mu$ (이놈은 우리가 배우고자하는 policy. 예를 들어 사람의 행동일 수도 있고, 좀 더 성능이 나은 agent의 policy일 수도 있음) 에서 $\\epsilon$-greedy 하게 뽑음\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\big( R + \\gamma \\max_{a'} Q(S',a') - Q(S,A) \\big)$$\n",
    "\n",
    "(Q-learning control은 최적 action-value function 으로 수렴한다는 것이 증명되어 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Approximation (by SGD)\n",
    "* Goal: true value function $v_\\pi (s)$ 가 있다고 할 때, 이걸 바로 구할 수 없으니깐 학습시킬 수 있는 parameter $\\mathbf{w}$ 를 써서, value function $\\hat{v}(s,\\mathbf{w})$ 를 사용해서 true value function 에 근사시키자. 이 때 MSE 를 써서 근사시킨다.\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\mathbb{E}_\\pi \\big[ (v_\\pi (S) - \\hat{v}(S, \\mathbf{w}))^2 \\big]\n",
    "$$\n",
    "\n",
    "* GD로 local minimum 찾으려면\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w} & = - {1 \\over 2} \\alpha \\nabla_w J(w)\\\\\n",
    "& = \\alpha \\mathbb{E}_\\pi [ (v_\\pi (S) - \\hat(v) (S, w))\\nabla_w \\hat{v} (S, w) ]\n",
    "\\end{align}\n",
    "\n",
    "(위에 있는 J(w) 첫 행에 그대로 대입, 알파는 미분할 놈 아니라 앞으로 나오고, V(s)도 w없어서 사라지고 둘째 줄 처럼 미분 결과 나옴)\n",
    "\n",
    "* 위 식에서 샘플링하면 expectation 사라짐 -> SGD\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (v_\\pi (S) - \\hat{v} (S, w)) \\nabla_w \\hat{v}(S,w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Prediction Algorithms\n",
    "* 위 w 업뎃 방법은 oracle만 아는 $v_\\pi (s)$ 를 우리도 안다고 생각하고 업뎃해야하는데\n",
    "* 현실 세계에서는 그걸 모름\n",
    "* 그래서 걍 $v_\\pi(s)$ 자리에 다가 관측한 리턴 $G_t$ 를 넣으면 됨. 즉,\n",
    "  * MC 라면 target 은 return $G_t$\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (G_t - \\hat{v} (S_t, w)) \\nabla_w \\hat{v}(S_t, w)\n",
    "  $$\n",
    "  \n",
    "  * TD(0) 이라면 TD target $R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w)$ 적용\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w) - \\hat{v} (S_t, w))\\nabla_w \\hat{v} (S_t, w)\n",
    "  $$\n",
    "  \n",
    "  * TD($\\lambda$)는 $\\lambda$-return $G_t^\\lambda$ 대입하면 됨\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (G_t^\\lambda - \\hat{v}(S_t, w))\\nabla_w \\hat{v}(S_t,w)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습 방법\n",
    "### Control\n",
    "$$\n",
    "L_i (\\theta_i) = \\mathbb{E}_{s,a,r,s'}\\sim U(D) \\Bigg[ \\Bigg( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s,a;\\theta_i) \\Bigg)^2 \\Bigg]\n",
    "$$\n",
    "\n",
    "* Behaviour policy 는 학습할 수 있도록 맨날 켜두고\n",
    "* Target policy 고정해두고 몇 iteration 마다 behaviour policy 복제해옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD 적용 하려면 위 control 식 미분\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_i} L(\\theta_i) = \\mathbb{E}_{s,a,r,s'}\\Bigg[ \\Bigg( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s,a;\\theta_i) \\Bigg) \\nabla_{\\theta_i}Q(s,a;\\theta_i) \\Bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q 함수는 CNN 모델로 function approximation 해버림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 안정성\n",
    "* RL에서 비선형 함수를 사용하면 학습 불안정 (발산하기도 함)\n",
    "* 왜냐면 observation 의 sequence에 있는 correlation 때문 (에피소드 끝날 때 까지 한 시쿼스 씩 가져다가 학습시키면 variance 커서 그런듯)\n",
    "$\\rightarrow$\n",
    "* 해결법\n",
    "  * experience replay\n",
    "  * target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "* 시뮬레이션 돌리며 매 틱(time step) 마다 생성되는 transition 튜플 $(s_t, a_t, r_t, s_{t+1})$을 replay buffer에 저장해둠\n",
    "* replay buffer에서 uniform 하게 sampling 해서 minibatch 가져다가 학습시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network\n",
    "(위에 요 내용)\n",
    "* Behaviour policy 는 학습할 수 있도록 맨날 켜두고\n",
    "* Target policy 고정해두고 몇 iteration 마다 behaviour policy 복제해옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Code\n",
    "(원래 이미지 전처리하는 과정도 있어야하지만 그건 빼고 적음)\n",
    "* Replay memory D와 총 개수 N을 초기화 함\n",
    "* action-value function Q를 초기화 함 (뉴럴넷의 weight $\\theta$를 초기화)\n",
    "* target action-value function $\\hat{Q}$의 weight 초기화 $\\theta^- = \\theta$\n",
    "\n",
    "**For episode = 1, M do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;시퀀스 $s_1 = \\{ x_1 \\}$와 preprocessed sequence $\\phi_1 = \\phi (s_1)$(이미지 처리용)을 초기화<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**For t=1, T do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\epsilon$-greedy 로 action $a_t$ 선택<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;시뮬 돌려서 state $s_{t+1}$ 이랑 reward $r_t$ 얻음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1} = s_t, a_t$ 얻음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transition $s_t, a_t, r_t, s_{t+1}$ 을 D에 넣음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D로부터 미니배치 샘플링함<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;target 정하는 과정<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) 만약 step j+1 에서 에피소드 끝났으면 $y_j = r_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) 아니면 $r_j + \\gamma \\max_{a'} \\hat{Q} (s_{t+1},a';\\theta^-)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;뉴럴넷 $\\theta$를 $\\big( y_j - Q(s_{j+1},a_j;\\theta) \\big)^2$<br> 미분해서 업뎃<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;매 C번의 step 지나면 $\\hat{Q} = Q$ 로 복사해줌<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End for**<br>\n",
    "**End for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 5e-4\n",
    "GAMMA = .98\n",
    "BUFFER_LIMIT = 50000\n",
    "BATCH_SIZE = 32\n",
    "EPISODES = 10000\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=BUFFER_LIMIT)\n",
    "        \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r)\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append(done_mask)\n",
    "            \n",
    "        return np.array(s_lst),np.array(a_lst),np.array(r_lst),np.array(s_prime_lst),np.array(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(models.Model):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.qnet = models.Sequential([\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(env.action_space.n)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.qnet(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        e = random.random()\n",
    "        if e < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            x = self.call(obs)\n",
    "            return tf.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "l2 = losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      " (2, 3) \n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "b\n",
      " (2,) \n",
      " [1 2]\n",
      "\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.cast([[1,2,3],[4,5,6]],dtype=tf.float32)\n",
    "b = tf.cast([1,2],dtype=tf.int32)\n",
    "\n",
    "print('a\\n', a.shape,'\\n', a.numpy())\n",
    "print()\n",
    "print('b\\n', b.shape, '\\n',b.numpy())\n",
    "print()\n",
    "print(tf.gather_nd(a,b).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory):\n",
    "    for i in range(EPOCHS):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            q_out = q(s)\n",
    "            \n",
    "            \n",
    "            max_q_prime = q_target(s_prime)\n",
    "            \n",
    "            target = r + GAMMA * max_q_prime * done_mask\n",
    "            loss = l2(q_out, target)\n",
    "            \n",
    "        grads = t.gradient(loss, q.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, q.trainable_variables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "score [0.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: shape=(32,), dtype=int64, numpy=\narray([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1])>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4b3694bd166d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# transition 몇 개 이상 모이면 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-0aa7b967fa1c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(q, q_target, memory)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmax_q_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m       \u001b[0m_check_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: shape=(32,), dtype=int64, numpy=\narray([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1])>"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "q = Qnet()\n",
    "q_target = Qnet()\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "score = .0\n",
    "\n",
    "copy = True\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    epsilon = max(0.01, 0.08 - 0.01*(e/200))\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 현재 state\n",
    "        s = tf.cast(s, dtype=tf.float32)\n",
    "        s = tf.expand_dims(s, axis=0)\n",
    "        s = tf.expand_dims(s, axis=0)\n",
    "        \n",
    "        # state로부터 a 고름\n",
    "        a = tf.squeeze(q.sample_action(s, epsilon))\n",
    "        s = tf.squeeze(s,axis=0)\n",
    "        \n",
    "        # (s,a) -> r, s' 구함\n",
    "        s_prime, r, done, _ = env.step(int(a))\n",
    "        done_mask = tf.zeros(1,) if done else tf.ones(1,)\n",
    "        s_prime = tf.expand_dims(s_prime,axis=0)\n",
    "        a = tf.expand_dims(a,axis=0)\n",
    "        r = tf.expand_dims(r,axis=0)\n",
    "        \n",
    "        # (s, a, r, s') transition 을 저장해줌\n",
    "        memory.put((s, a, r, s_prime, done_mask))\n",
    "        \n",
    "        # 다음 state s' 가 이제 현재 s가 됨\n",
    "        s = tf.squeeze(s_prime)\n",
    "        \n",
    "        # 점수 확인용\n",
    "        score += r.numpy()\n",
    "        \n",
    "        # 에피소드 끝\n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "        if copy:\n",
    "            q(memory.sample(1)[0])\n",
    "            q_target(memory.sample(1)[0])\n",
    "            q_target.set_weights(q.get_weights())\n",
    "            copy = False\n",
    "                        \n",
    "    # transition 몇 개 이상 모이면 학습\n",
    "    if memory.size() > 2000:\n",
    "        train(q, q_target, memory)\n",
    "        \n",
    "    if e != 0 and e%20 == 0:\n",
    "        ipd.clear_output(wait=True)\n",
    "        print(e)\n",
    "\n",
    "        print('score',score)\n",
    "        score = .0\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 확인\n",
    "s = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # 현재 state\n",
    "    s = tf.cast(s, dtype=tf.float32)\n",
    "    s = tf.expand_dims(s, axis=0)\n",
    "    s = tf.expand_dims(s, axis=0)\n",
    "\n",
    "    # state로부터 a 고름\n",
    "    a = tf.squeeze(q.sample_action(s, epsilon))\n",
    "    s = tf.squeeze(s,axis=0)\n",
    "\n",
    "    # (s,a) -> r, s' 구함\n",
    "    s_prime, r, done, _ = env.step(int(a))\n",
    "    env.render()\n",
    "    done_mask = tf.zeros(1,) if done else tf.ones(1,)\n",
    "    s_prime = tf.expand_dims(s_prime,axis=0)\n",
    "    a = tf.expand_dims(a,axis=0)\n",
    "    r = tf.expand_dims(r,axis=0)\n",
    "    \n",
    "    # 에피소드 끝\n",
    "    if done: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
