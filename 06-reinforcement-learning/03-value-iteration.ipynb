{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 목표: optimal policy $\\pi$ 찾기\n",
    "* 해법: Bellman optimality backup을 반복적으로 적용\n",
    "* $v_1 \\rightarrow v_2 \\rightarrow \\cdots \\rightarrow v_*$\n",
    "* Synchronous backpu 사용\n",
    "  * 각각의 iteration $k+1$ 에서\n",
    "  * 모든 state $s\\in S$ 에 대해\n",
    "  * $v_k (s')$ 로부터 $v_{k+1}(s)$ 를 업뎃\n",
    "* $v_*$ 가 수렴한다는 것은 증명되어 있음\n",
    "* Policy iteration 과 달리, explicity 한 policy 없음\n",
    "* 중간 단계의 value function 은 어떤 policy 에 관한 것도 아닐 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "v_{k+1} (s) & = \\max_{a\\in A}\\Big( R_s^a + \\gamma \\sum_{s' \\in S} P^a_{ss'} v_k \\big(s' \\big) \\Big)\\\\\n",
    "v_{k+1} & = \\max_{a\\in A} R^a + \\gamma P^a v_k\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "* Frozen lake 환경을 세팅\n",
    "* Gym 에서 discrete 한 environment 쓸만한 놈이 이거라서 가져옴\n",
    "* 'S' 타일에서 시작\n",
    "* Action으로 'Up', 'Left', 'Right', 'Down'을 줄 수 있음\n",
    "* 에피소드는 'H' 타일에 빠져서 reward 0 (죽음) 얻거나, 'G' (goal) 타일로 가서 reward 1 (점수) 얻거나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```is_slippery``` 를 True, False에 따라\n",
    "* True: action에 따라 확정적으로 이동하지 않고, 확률적으로 action 말고 다른데로 가버림 (frozen lake라 미끄러워서 그렇다고 함...)\n",
    "* False: action 에 따라 확정적으로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state S space: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "state S' space: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "action A space: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "S = env.observation_space.n\n",
    "S_ = env.observation_space.n\n",
    "A = env.action_space.n\n",
    "print('state S space:', np.arange(S))\n",
    "print('state S\\' space:', np.arange(S_))\n",
    "print('action A space:', np.arange(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤하게 움직여서 겜 한판 돌려보자!\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(\"랜덤하게 움직여서 겜 한판 돌려보자!\")\n",
    "s = env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "while True:\n",
    "    s, r, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constant\n",
    "* $\\gamma$ : discount factor\n",
    "* $\\theta$ : policy evaluation threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = .9 # discount factor\n",
    "EPISODES = 1000 # policy evaluation 끝나고 transition matrix 업데이트 할 횟수\n",
    "THETA = 1e-4 # policy evaluation threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\pi$ : policy. state에 따라 어떤 action 취할지 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init policy, S x A x 1\n",
    "pi = np.ones((S, A))\n",
    "pi /= A\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 리워드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init rewrads, S x A x 1\n",
    "rewards = np.zeros((S, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transition (model 이라고도 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init transition matrix, A x S x S' x 1\n",
    "Pss = np.zeros((A, S, S_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* value function : 이 예제에서는 state-value function 을 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init value function, S x 1\n",
    "valuefunction = np.ones((S)) / S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list\n",
    "# return_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration, for estimating $\\pi \\approx \\pi_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm parameter: 정확도 estimation 을 위한 threshold $\\theta>0$<br/>\n",
    "모든 $s\\in S^+$에 대한 $V(s)$를 임의의 값으로 초기화, 단 $V(terminal)=0$으로<br/><br/>\n",
    "Loop:</br>\n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;$\\Delta \\leftarrow 0$</br>\n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;Loop for each $s\\in S$:</br>\n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v \\leftarrow V(s)$</br>\n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s) \\leftarrow \\max_a \\sum_{s', r}p(s',r|s,a)[r+\\gamma V(s')]$</br>\n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\Delta \\leftarrow \\max (\\Delta, |v - V(s)|)$</br>\n",
    "until $\\Delta < \\theta$<br/><br/>\n",
    "아래를 만족하는 deterministic policy, $\\pi \\approx \\pi_*$ 를 구함</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\pi (s) = \\arg \\max_a \\sum_{s', r}p(s', r|s, a)[r+\\gamma V(s')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.arange(A)\n",
    "\n",
    "def get_action(state):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        state: 상태\n",
    "    return: \n",
    "        policy 의 확률에 따라 취할 수 있는 action 중에서 하나 구해서 리턴\n",
    "    \"\"\"\n",
    "    return np.random.choice(action, size=None, p=pi[state])\n",
    "\n",
    "def get_transition(episodes):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        episodes 수\n",
    "    return: \n",
    "        None\n",
    "        에피소드 수 주어진 만큼 environment 돌아다니면서 transition matrix 채움\n",
    "    \"\"\"\n",
    "    global rewards, return_list\n",
    "    \n",
    "    _rewards = np.zeros(rewards.shape)\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = get_action(state)\n",
    "            state_, _reward, done, _ = env.step(action)\n",
    "            \n",
    "            Pss[action][state][state_] += 1 # cumulate again and again\n",
    "            _rewards[state][action] += _reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = state_\n",
    "                \n",
    "    # average rewards\n",
    "    _rewards /= episodes\n",
    "    rewards = _rewards\n",
    "#     return_list.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 구현은 \n",
    "\n",
    "$$\n",
    "v_{k+1}(s) = \\max_{a\\in A} \\big( R_s^a + \\gamma \\sum_{s' \\in S} P^a_{ss'} v_k (s') \\big)\n",
    "$$\n",
    "\n",
    "요걸로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE ITERATION\n",
    "\n",
    "# 에피소드 수 만큼 돌면서 transition matrix를 채움\n",
    "get_transition(episodes=100)\n",
    "\n",
    "while True:\n",
    "    # loop 멈출 현재의 threshold\n",
    "    delta = .0\n",
    "    \n",
    "    pss_sum = np.sum(Pss)\n",
    "    _Pss = Pss/pss_sum # 수정해야함\n",
    "    PssV = np.matmul(_Pss, valuefunction)\n",
    "    PssV *= GAMMA # (A,S)\n",
    "    \n",
    "    _valuefunction = PssV.T + rewards\n",
    "    \n",
    "    newV = np.zeros(valuefunction.shape)\n",
    "    \n",
    "    for s in range(S):\n",
    "        newV[s] = np.max(_valuefunction[s])\n",
    "            \n",
    "    delta = max(delta, np.sum(np.abs(valuefunction - newV)))\n",
    "    \n",
    "    valuefunction = np.array(newV)\n",
    "\n",
    "    if delta < THETA:\n",
    "        break\n",
    "\n",
    "    # 그 담번엔 EPISODES번씩\n",
    "    get_transition(EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래를 만족하는 deterministic policy, $\\pi \\approx \\pi_*$ 를 구함</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\pi (s) = \\arg \\max_a \\sum_{s', r}p(s', r|s, a)[r+\\gamma V(s')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi (S, A) = (16, 4) , Pss (A, S, S') = (4, 16, 16) , reward (S, A) (16, 4)\n"
     ]
    }
   ],
   "source": [
    "print('pi (S, A) =', pi.shape, ', Pss (A, S, S\\') =', Pss.shape, ', reward (S, A)', rewards.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax_a 만족하는 a만 1이고, 나머지는 0 되도록 하기 위해서\n",
    "pi = np.zeros(pi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pss_sum = np.sum(Pss)\n",
    "_Pss = Pss/pss_sum\n",
    "PssV = np.matmul(_Pss, valuefunction)\n",
    "PssV *= GAMMA\n",
    "\n",
    "_valuefunction = PssV.T + rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(S):\n",
    "    pi[s][np.argmax(_valuefunction[s])] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration으로 찾은 policy에 따라 겜 해보자!\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy iteration으로 찾은 policy에 따라 겜 해보자!\")\n",
    "s = env.reset()\n",
    "env.render()\n",
    "\n",
    "while True:\n",
    "    s, r, done, _ = env.step(get_action(s))\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print('reward:',r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in range(A):\n",
    "#     for s in range(S):\n",
    "#         for s_ in range(S):\n",
    "#             print(f\"(a:{a}, s:{s}) -> s':{s_} : {Pss[a][s][s_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration 으로 풀었을 때, 에이전트 퍼포먼스: 1000회 중 1000회 성공 -> 100.00%\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "cnt_success = 0\n",
    "trial = 1000\n",
    "\n",
    "for _ in range(trial):\n",
    "    while True:\n",
    "        s, r, done, _ = env.step(get_action(s))\n",
    "        cnt_success += r\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            break\n",
    "        \n",
    "print(f\"policy iteration 으로 풀었을 때, 에이전트 퍼포먼스: {trial}회 중 {cnt_success:n}회 성공 -> {cnt_success/trial * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아무곳이나 가는 에이전트 퍼포먼스: 1000회 중 13회 성공 -> 1.30%\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "cnt_success = 0\n",
    "trial = 1000\n",
    "\n",
    "for _ in range(trial):\n",
    "    while True:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "        cnt_success += r\n",
    "\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            break\n",
    "        \n",
    "print(f\"아무곳이나 가는 에이전트 퍼포먼스: {trial}회 중 {cnt_success:n}회 성공 -> {cnt_success/trial * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
