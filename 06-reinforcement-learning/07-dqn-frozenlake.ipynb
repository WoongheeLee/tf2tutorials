{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-Level Control Through Deep Reinforcement Learning\n",
    "* DQN\n",
    "* nature 2015\n",
    "* 구글 딥마인드 연구\n",
    "* DQN 논문 리뷰 영상 https://www.youtube.com/watch?v=eJXQKEtPvhY 의 슬라이드 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "* input: 게임화면을 state로 주고, 게임 점수는 reward로 줌\n",
    "* output: reward 기대값 최대가 되는 policy 찾기\n",
    "* 구체적인 state를 주지않고, 게임 pixel만 줘서 사람보다 게임 잘하는 agent 를 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사전 지식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "* TD target은 우리가 학습시키려는 policy $\\pi$에서 greedy 하게 뽑고\n",
    "\n",
    "$$\n",
    "\\pi (S_{t+1}) = \\arg \\max_{a'} Q(S_{t+1}, a')\n",
    "$$\n",
    "\n",
    "* 현재 value는 behaviour policy $\\mu$ (이놈은 우리가 배우고자하는 policy. 예를 들어 사람의 행동일 수도 있고, 좀 더 성능이 나은 agent의 policy일 수도 있음) 에서 $\\epsilon$-greedy 하게 뽑음\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\big( R + \\gamma \\max_{a'} Q(S',a') - Q(S,A) \\big)$$\n",
    "\n",
    "(Q-learning control은 최적 action-value function 으로 수렴한다는 것이 증명되어 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Approximation (by SGD)\n",
    "* Goal: true value function $v_\\pi (s)$ 가 있다고 할 때, 이걸 바로 구할 수 없으니깐 학습시킬 수 있는 parameter $\\mathbf{w}$ 를 써서, value function $\\hat{v}(s,\\mathbf{w})$ 를 사용해서 true value function 에 근사시키자. 이 때 MSE 를 써서 근사시킨다.\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\mathbb{E}_\\pi \\big[ (v_\\pi (S) - \\hat{v}(S, \\mathbf{w}))^2 \\big]\n",
    "$$\n",
    "\n",
    "* GD로 local minimum 찾으려면\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w} & = - {1 \\over 2} \\alpha \\nabla_w J(w)\\\\\n",
    "& = \\alpha \\mathbb{E}_\\pi [ (v_\\pi (S) - \\hat(v) (S, w))\\nabla_w \\hat{v} (S, w) ]\n",
    "\\end{align}\n",
    "\n",
    "(위에 있는 J(w) 첫 행에 그대로 대입, 알파는 미분할 놈 아니라 앞으로 나오고, V(s)도 w없어서 사라지고 둘째 줄 처럼 미분 결과 나옴)\n",
    "\n",
    "* 위 식에서 샘플링하면 expectation 사라짐 -> SGD\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (v_\\pi (S) - \\hat{v} (S, w)) \\nabla_w \\hat{v}(S,w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Prediction Algorithms\n",
    "* 위 w 업뎃 방법은 oracle만 아는 $v_\\pi (s)$ 를 우리도 안다고 생각하고 업뎃해야하는데\n",
    "* 현실 세계에서는 그걸 모름\n",
    "* 그래서 걍 $v_\\pi(s)$ 자리에 다가 관측한 리턴 $G_t$ 를 넣으면 됨. 즉,\n",
    "  * MC 라면 target 은 return $G_t$\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (G_t - \\hat{v} (S_t, w)) \\nabla_w \\hat{v}(S_t, w)\n",
    "  $$\n",
    "  \n",
    "  * TD(0) 이라면 TD target $R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w)$ 적용\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w) - \\hat{v} (S_t, w))\\nabla_w \\hat{v} (S_t, w)\n",
    "  $$\n",
    "  \n",
    "  * TD($\\lambda$)는 $\\lambda$-return $G_t^\\lambda$ 대입하면 됨\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (G_t^\\lambda - \\hat{v}(S_t, w))\\nabla_w \\hat{v}(S_t,w)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습 방법\n",
    "### Control\n",
    "$$\n",
    "L_i (\\theta_i) = \\mathbb{E}_{s,a,r,s'}\\sim U(D) \\Bigg[ \\Bigg( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s,a;\\theta_i) \\Bigg)^2 \\Bigg]\n",
    "$$\n",
    "\n",
    "* Behaviour policy 는 학습할 수 있도록 맨날 켜두고\n",
    "* Target policy 고정해두고 몇 iteration 마다 behaviour policy 복제해옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD 적용 하려면 위 control 식 미분\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_i} L(\\theta_i) = \\mathbb{E}_{s,a,r,s'}\\Bigg[ \\Bigg( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s,a;\\theta_i) \\Bigg) \\nabla_{\\theta_i}Q(s,a;\\theta_i) \\Bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q 함수는 CNN 모델로 function approximation 해버림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 안정성\n",
    "* RL에서 비선형 함수를 사용하면 학습 불안정 (발산하기도 함)\n",
    "* 왜냐면 observation 의 sequence에 있는 correlation 때문 (에피소드 끝날 때 까지 한 시쿼스 씩 가져다가 학습시키면 variance 커서 그런듯)\n",
    "$\\rightarrow$\n",
    "* 해결법\n",
    "  * experience replay\n",
    "  * target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "* 시뮬레이션 돌리며 매 틱(time step) 마다 생성되는 transition 튜플 $(s_t, a_t, r_t, s_{t+1})$을 replay buffer에 저장해둠\n",
    "* replay buffer에서 uniform 하게 sampling 해서 minibatch 가져다가 학습시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network\n",
    "(위에 요 내용)\n",
    "* Behaviour policy 는 학습할 수 있도록 맨날 켜두고\n",
    "* Target policy 고정해두고 몇 iteration 마다 behaviour policy 복제해옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Code\n",
    "(원래 이미지 전처리하는 과정도 있어야하지만 그건 빼고 적음)\n",
    "* Replay memory D와 총 개수 N을 초기화 함\n",
    "* action-value function Q를 초기화 함 (뉴럴넷의 weight $\\theta$를 초기화)\n",
    "* target action-value function $\\hat{Q}$의 weight 초기화 $\\theta^- = \\theta$\n",
    "\n",
    "**For episode = 1, M do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;시퀀스 $s_1 = \\{ x_1 \\}$와 preprocessed sequence $\\phi_1 = \\phi (s_1)$(이미지 처리용)을 초기화<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**For t=1, T do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\epsilon$-greedy 로 action $a_t$ 선택<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;시뮬 돌려서 state $s_{t+1}$ 이랑 reward $r_t$ 얻음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1} = s_t, a_t$ 얻음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transition $s_t, a_t, r_t, s_{t+1}$ 을 D에 넣음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D로부터 미니배치 샘플링함<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;target 정하는 과정<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) 만약 step j+1 에서 에피소드 끝났으면 $y_j = r_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) 아니면 $r_j + \\gamma \\max_{a'} \\hat{Q} (s_{t+1},a';\\theta^-)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;뉴럴넷 $\\theta$를 $\\big( y_j - Q(s_{j+1},a_j;\\theta) \\big)^2$<br> 미분해서 업뎃<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;매 C번의 step 지나면 $\\hat{Q} = Q$ 로 복사해줌<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End for**<br>\n",
    "**End for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 1e-3\n",
    "GAMMA = .95\n",
    "BUFFER_LIMIT = 10000\n",
    "BATCH_SIZE = 1\n",
    "EPISODES = 10000\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        총 길이가 제한이 있는 que만들기\n",
    "        '''\n",
    "        self.buffer = collections.deque(maxlen=BUFFER_LIMIT)\n",
    "        \n",
    "    def put(self, transition):\n",
    "        '''\n",
    "        args:\n",
    "            transitions: s, a, r, s', done(종료 step인지 아닌지 확인용)\n",
    "        '''\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        '''\n",
    "        args:\n",
    "            샘플링할 개수 n\n",
    "        return:\n",
    "            replay buffer에 저장된 transition 중 n개 random sampling\n",
    "        '''\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r)\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append(done_mask)\n",
    "            \n",
    "        return np.array(s_lst),np.array(a_lst),np.array(r_lst),np.array(s_prime_lst),np.array(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        '''\n",
    "        replay buffer 현재 크기 확인용\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(models.Model):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        3층 layer로 간단히 구현\n",
    "        최종 layer는 value (real value 범위) 를 출력해야 하므로 activation function 없음\n",
    "        '''\n",
    "        super(Qnet, self).__init__()\n",
    "        self.qnet = models.Sequential([\n",
    "            layers.Dense(32),\n",
    "            layers.Dense(env.action_space.n)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        '''\n",
    "        args:\n",
    "            state\n",
    "        return:\n",
    "            value (given s, a지만 a는 명시적으로 주지 않음)\n",
    "        '''\n",
    "        x = self.qnet(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        '''\n",
    "        args:\n",
    "            state\n",
    "        return:\n",
    "            epsilon greedy로 action 선택\n",
    "        '''\n",
    "        e = random.random()\n",
    "        if e < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            x = tf.squeeze(self.call(obs))\n",
    "            return tf.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "loss_func = losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory):\n",
    "    '''\n",
    "    args:\n",
    "        q: behaviour policy = mu\n",
    "        q_target: target policy = pi\n",
    "        memory: replay memory\n",
    "    return:\n",
    "        None\n",
    "        모델 학습시킴\n",
    "    '''\n",
    "    for i in range(EPOCHS):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(BATCH_SIZE)\n",
    "\n",
    "        with tf.GradientTape() as t:\n",
    "            # state에 따른 value 뽑아냄\n",
    "            q_out = q(s, training=True)\n",
    "            q_out = tf.multiply(q_out, a)\n",
    "            # 그 중에서 action 취한 value만 뽑아냄\n",
    "            q_out = tf.reduce_max(q_out, axis=-1)\n",
    "\n",
    "            # target, s'에 대한 value 계산\n",
    "            max_q_prime = q_target(s_prime)\n",
    "            # 그 중에서 max 인 value만 뽑아냄\n",
    "            max_q_prime = tf.reduce_max(max_q_prime, axis=-1)\n",
    "\n",
    "            target = r + GAMMA * max_q_prime * done_mask\n",
    "            loss = loss_func(q_out, target)\n",
    "\n",
    "        grads = t.gradient(loss, q.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, q.trainable_variables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon: 1 부터 시작해서 최종 EPISODES 까지 선형적으로 .1이 되도록 줄어듦\n",
    "f = lambda x: max(.8 - 1/EPISODES*x, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "q = Qnet()\n",
    "q_target = Qnet()\n",
    "memory = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_state(s):\n",
    "    '''\n",
    "    args:\n",
    "        state\n",
    "    return:\n",
    "        one-hot encoded state\n",
    "    '''\n",
    "    _s = np.zeros((env.observation_space.n,))\n",
    "    _s[s] = 1.\n",
    "    return tf.expand_dims(tf.cast(_s, dtype=tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbfec2334d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# theta로 부터 theta- 초기화 때 한 번만 복사할 flag\n",
    "copy = True\n",
    "\n",
    "r_list = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    epsilon = f(e)\n",
    "    s = env.reset()\n",
    "    s = onehot_state(s)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 주어진 state로부터 action 구함\n",
    "        a = tf.squeeze(q.sample_action(s, epsilon))\n",
    "        \n",
    "        # (s,a) -> r, s' 구함\n",
    "        s_prime, r, done, _ = env.step(int(a))\n",
    "        done_mask = tf.zeros(1,) if done else tf.ones(1,)\n",
    "        \n",
    "        # action 을 one-hot encoding 해서 transition 에 저장\n",
    "        _a = np.zeros((env.action_space.n,))\n",
    "        _a[int(a)] = 1.\n",
    "        a = tf.cast(_a, dtype=tf.float32)\n",
    "        r = tf.expand_dims(r,axis=0)\n",
    "        \n",
    "        # state -> one-hot encoding 해서 transition 에 저장\n",
    "        s_prime = onehot_state(s_prime)\n",
    "        \n",
    "        # (s, a, r, s') transition 을 저장해줌\n",
    "        memory.put((s, a, r, s_prime, done_mask))\n",
    "        \n",
    "        # 다음 state s' 가 이제 현재 s가 됨\n",
    "        s = s_prime.numpy()\n",
    "        \n",
    "        # 에피소드 끝\n",
    "        if done: \n",
    "            s = env.reset()\n",
    "            s = onehot_state(s)\n",
    "            r_list.append(np.squeeze(r))\n",
    "            break\n",
    "            \n",
    "        if copy:\n",
    "            q(memory.sample(1)[0])\n",
    "            q_target(memory.sample(1)[0])\n",
    "            q_target.set_weights(q.get_weights())\n",
    "            copy = False\n",
    "                        \n",
    "    # transition 몇 개 이상 모이면 학습\n",
    "    # 성공 경험이 있을 때 학습\n",
    "    if memory.size() > BATCH_SIZE and np.sum(r_list) > 0:\n",
    "        train(q, q_target, memory)\n",
    "        \n",
    "    if e != 0 and e%50 == 0:\n",
    "        \n",
    "        # 가끔 behaviour 를 target에 복사해줌\n",
    "        q_target.set_weights(q.get_weights())\n",
    "        \n",
    "    ipd.clear_output(wait=True)\n",
    "    plt.figure(facecolor='w',figsize=(10,1))\n",
    "    plt.plot(r_list)\n",
    "    plt.title(f\"{e+1}/{EPISODES}, {np.sum(r_list[-50:])/50*100:.4f}%, e={epsilon:.2f}\")\n",
    "    plt.show()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "TEST_EPISODES = 100\n",
    "for i in range(TEST_EPISODES):\n",
    "    s = env.reset()\n",
    "    s = onehot_state(s)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # 주어진 state로부터 action 구함\n",
    "        a = tf.squeeze(q.sample_action(s, 0.0))\n",
    "\n",
    "        # (s,a) -> r, s' 구함\n",
    "        s_prime, r, done, _ = env.step(int(a))\n",
    "        \n",
    "        # state -> one-hot encoding 해서 transition 에 저장\n",
    "        s_prime = onehot_state(s_prime)\n",
    "\n",
    "        # 다음 state s' 가 이제 현재 s가 됨\n",
    "        s = s_prime.numpy()\n",
    "\n",
    "        # 에피소드 끝\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            s = onehot_state(s)\n",
    "            break\n",
    "\n",
    "    acc += np.squeeze(r)\n",
    "    \n",
    "print(f\"{np.squeeze(acc)/TEST_EPISODES*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.save_weights('./model/dqn-frozenlake')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
