{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-Level Control Through Deep Reinforcement Learning\n",
    "* DQN\n",
    "* nature 2015\n",
    "* 구글 딥마인드 연구\n",
    "* DQN 논문 리뷰 영상 https://www.youtube.com/watch?v=eJXQKEtPvhY 의 슬라이드 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "* input: 게임화면을 state로 주고, 게임 점수는 reward로 줌\n",
    "* output: reward 기대값 최대가 되는 policy 찾기\n",
    "* 구체적인 state를 주지않고, 게임 pixel만 줘서 사람보다 게임 잘하는 agent 를 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사전 지식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "* TD target은 우리가 학습시키려는 policy $\\pi$에서 greedy 하게 뽑고\n",
    "\n",
    "$$\n",
    "\\pi (S_{t+1}) = \\arg \\max_{a'} Q(S_{t+1}, a')\n",
    "$$\n",
    "\n",
    "* 현재 value는 behaviour policy $\\mu$ (이놈은 우리가 배우고자하는 policy. 예를 들어 사람의 행동일 수도 있고, 좀 더 성능이 나은 agent의 policy일 수도 있음) 에서 $\\epsilon$-greedy 하게 뽑음\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\big( R + \\gamma \\max_{a'} Q(S',a') - Q(S,A) \\big)$$\n",
    "\n",
    "(Q-learning control은 최적 action-value function 으로 수렴한다는 것이 증명되어 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Approximation (by SGD)\n",
    "* Goal: true value function $v_\\pi (s)$ 가 있다고 할 때, 이걸 바로 구할 수 없으니깐 학습시킬 수 있는 parameter $\\mathbf{w}$ 를 써서, value function $\\hat{v}(s,\\mathbf{w})$ 를 사용해서 true value function 에 근사시키자. 이 때 MSE 를 써서 근사시킨다.\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\mathbb{E}_\\pi \\big[ (v_\\pi (S) - \\hat{v}(S, \\mathbf{w}))^2 \\big]\n",
    "$$\n",
    "\n",
    "* GD로 local minimum 찾으려면\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w} & = - {1 \\over 2} \\alpha \\nabla_w J(w)\\\\\n",
    "& = \\alpha \\mathbb{E}_\\pi [ (v_\\pi (S) - \\hat(v) (S, w))\\nabla_w \\hat{v} (S, w) ]\n",
    "\\end{align}\n",
    "\n",
    "(위에 있는 J(w) 첫 행에 그대로 대입, 알파는 미분할 놈 아니라 앞으로 나오고, V(s)도 w없어서 사라지고 둘째 줄 처럼 미분 결과 나옴)\n",
    "\n",
    "* 위 식에서 샘플링하면 expectation 사라짐 -> SGD\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (v_\\pi (S) - \\hat{v} (S, w)) \\nabla_w \\hat{v}(S,w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Prediction Algorithms\n",
    "* 위 w 업뎃 방법은 oracle만 아는 $v_\\pi (s)$ 를 우리도 안다고 생각하고 업뎃해야하는데\n",
    "* 현실 세계에서는 그걸 모름\n",
    "* 그래서 걍 $v_\\pi(s)$ 자리에 다가 관측한 리턴 $G_t$ 를 넣으면 됨. 즉,\n",
    "  * MC 라면 target 은 return $G_t$\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (G_t - \\hat{v} (S_t, w)) \\nabla_w \\hat{v}(S_t, w)\n",
    "  $$\n",
    "  \n",
    "  * TD(0) 이라면 TD target $R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w)$ 적용\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w) - \\hat{v} (S_t, w))\\nabla_w \\hat{v} (S_t, w)\n",
    "  $$\n",
    "  \n",
    "  * TD($\\lambda$)는 $\\lambda$-return $G_t^\\lambda$ 대입하면 됨\n",
    "  \n",
    "  $$\n",
    "  \\Delta w = \\alpha (G_t^\\lambda - \\hat{v}(S_t, w))\\nabla_w \\hat{v}(S_t,w)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습 방법\n",
    "### Control\n",
    "$$\n",
    "L_i (\\theta_i) = \\mathbb{E}_{s,a,r,s'}\\sim U(D) \\Bigg[ \\Bigg( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s,a;\\theta_i) \\Bigg)^2 \\Bigg]\n",
    "$$\n",
    "\n",
    "* Behaviour policy 는 학습할 수 있도록 맨날 켜두고\n",
    "* Target policy 고정해두고 몇 iteration 마다 behaviour policy 복제해옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD 적용 하려면 위 control 식 미분\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_i} L(\\theta_i) = \\mathbb{E}_{s,a,r,s'}\\Bigg[ \\Bigg( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s,a;\\theta_i) \\Bigg) \\nabla_{\\theta_i}Q(s,a;\\theta_i) \\Bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q 함수는 CNN 모델로 function approximation 해버림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 안정성\n",
    "* RL에서 비선형 함수를 사용하면 학습 불안정 (발산하기도 함)\n",
    "* 왜냐면 observation 의 sequence에 있는 correlation 때문 (에피소드 끝날 때 까지 한 시쿼스 씩 가져다가 학습시키면 variance 커서 그런듯)\n",
    "$\\rightarrow$\n",
    "* 해결법\n",
    "  * experience replay\n",
    "  * target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "* 시뮬레이션 돌리며 매 틱(time step) 마다 생성되는 transition 튜플 $(s_t, a_t, r_t, s_{t+1})$을 replay buffer에 저장해둠\n",
    "* replay buffer에서 uniform 하게 sampling 해서 minibatch 가져다가 학습시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network\n",
    "(위에 요 내용)\n",
    "* Behaviour policy 는 학습할 수 있도록 맨날 켜두고\n",
    "* Target policy 고정해두고 몇 iteration 마다 behaviour policy 복제해옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Code\n",
    "(원래 이미지 전처리하는 과정도 있어야하지만 그건 빼고 적음)\n",
    "* Replay memory D와 총 개수 N을 초기화 함\n",
    "* action-value function Q를 초기화 함 (뉴럴넷의 weight $\\theta$를 초기화)\n",
    "* target action-value function $\\hat{Q}$의 weight 초기화 $\\theta^- = \\theta$\n",
    "\n",
    "**For episode = 1, M do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;시퀀스 $s_1 = \\{ x_1 \\}$와 preprocessed sequence $\\phi_1 = \\phi (s_1)$(이미지 처리용)을 초기화<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**For t=1, T do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\epsilon$-greedy 로 action $a_t$ 선택<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;시뮬 돌려서 state $s_{t+1}$ 이랑 reward $r_t$ 얻음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s_{t+1} = s_t, a_t$ 얻음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transition $s_t, a_t, r_t, s_{t+1}$ 을 D에 넣음<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D로부터 미니배치 샘플링함<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;target 정하는 과정<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) 만약 step j+1 에서 에피소드 끝났으면 $y_j = r_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) 아니면 $r_j + \\gamma \\max_{a'} \\hat{Q} (s_{t+1},a';\\theta^-)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;뉴럴넷 $\\theta$를 $\\big( y_j - Q(s_{j+1},a_j;\\theta) \\big)^2$<br> 미분해서 업뎃<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;매 C번의 step 지나면 $\\hat{Q} = Q$ 로 복사해줌<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End for**<br>\n",
    "**End for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 1e-3\n",
    "GAMMA = .95\n",
    "BUFFER_LIMIT = 100000\n",
    "BATCH_SIZE = 32\n",
    "EPISODES = 10000\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        총 길이가 제한이 있는 que만들기\n",
    "        '''\n",
    "        self.buffer = collections.deque(maxlen=BUFFER_LIMIT)\n",
    "        \n",
    "    def put(self, transition):\n",
    "        '''\n",
    "        args:\n",
    "            transitions: s, a, r, s', done(종료 step인지 아닌지 확인용)\n",
    "        '''\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        '''\n",
    "        args:\n",
    "            샘플링할 개수 n\n",
    "        return:\n",
    "            replay buffer에 저장된 transition 중 n개 random sampling\n",
    "        '''\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r)\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append(done_mask)\n",
    "            \n",
    "        return np.array(s_lst),np.array(a_lst),np.array(r_lst),np.array(s_prime_lst),np.array(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        '''\n",
    "        replay buffer 현재 크기 확인용\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(models.Model):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        3층 layer로 간단히 구현\n",
    "        최종 layer는 value (real value 범위) 를 출력해야 하므로 activation function 없음\n",
    "        '''\n",
    "        super(Qnet, self).__init__()\n",
    "        self.qnet = models.Sequential([\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(env.action_space.n)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        '''\n",
    "        args:\n",
    "            state\n",
    "        return:\n",
    "            value (given s, a지만 a는 명시적으로 주지 않음)\n",
    "        '''\n",
    "        x = self.qnet(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        '''\n",
    "        args:\n",
    "            state\n",
    "        return:\n",
    "            epsilon greedy로 action 선택\n",
    "        '''\n",
    "        e = random.random()\n",
    "        if e < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            x = tf.squeeze(self.call(obs))\n",
    "            return tf.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "loss_func = losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory):\n",
    "    '''\n",
    "    args:\n",
    "        q: behaviour policy = mu\n",
    "        q_target: target policy = pi\n",
    "        memory: replay memory\n",
    "    return:\n",
    "        None\n",
    "        모델 학습시킴\n",
    "    '''\n",
    "    for i in range(EPOCHS):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(BATCH_SIZE)\n",
    "\n",
    "        with tf.GradientTape() as t:\n",
    "            # state에 따른 value 뽑아냄\n",
    "            q_out = q(s, training=True)\n",
    "            q_out = tf.multiply(q_out, a)\n",
    "            # 그 중에서 action 취한 value만 뽑아냄\n",
    "            q_out = tf.reduce_max(q_out, axis=-1)\n",
    "\n",
    "            # target, s'에 대한 value 계산\n",
    "            max_q_prime = q_target(s_prime)\n",
    "            # 그 중에서 max 인 value만 뽑아냄\n",
    "            max_q_prime = tf.reduce_max(max_q_prime, axis=-1)\n",
    "\n",
    "            target = r + GAMMA * max_q_prime * done_mask\n",
    "            loss = loss_func(q_out, target)\n",
    "\n",
    "        grads = t.gradient(loss, q.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, q.trainable_variables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon: 1 부터 시작해서 최종 EPISODES 까지 선형적으로 .1이 되도록 줄어듦\n",
    "f = lambda x: max(.8 - 1/EPISODES*x, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "q = Qnet()\n",
    "q_target = Qnet()\n",
    "memory = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_state(s):\n",
    "    '''\n",
    "    args:\n",
    "        state\n",
    "    return:\n",
    "        one-hot encoded state\n",
    "    '''\n",
    "    _s = np.zeros((env.observation_space.n,))\n",
    "    _s[s] = 1.\n",
    "    return tf.expand_dims(tf.cast(_s, dtype=tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# theta로 부터 theta- 초기화 때 한 번만 복사할 flag\n",
    "copy = True\n",
    "\n",
    "r_list = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    epsilon = f(e)\n",
    "    s = env.reset()\n",
    "    s = onehot_state(s)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 주어진 state로부터 action 구함\n",
    "        a = tf.squeeze(q.sample_action(s, epsilon))\n",
    "        \n",
    "        # (s,a) -> r, s' 구함\n",
    "        s_prime, r, done, _ = env.step(int(a))\n",
    "        done_mask = tf.zeros(1,) if done else tf.ones(1,)\n",
    "        \n",
    "        # action 을 one-hot encoding 해서 transition 에 저장\n",
    "        _a = np.zeros((env.action_space.n,))\n",
    "        _a[int(a)] = 1.\n",
    "        a = tf.cast(_a, dtype=tf.float32)\n",
    "        r = tf.expand_dims(r,axis=0)\n",
    "        \n",
    "        # state -> one-hot encoding 해서 transition 에 저장\n",
    "        s_prime = onehot_state(s_prime)\n",
    "        \n",
    "        # (s, a, r, s') transition 을 저장해줌\n",
    "        memory.put((s, a, r, s_prime, done_mask))\n",
    "        \n",
    "        # 다음 state s' 가 이제 현재 s가 됨\n",
    "        s = s_prime.numpy()\n",
    "        \n",
    "        # 에피소드 끝\n",
    "        if done: \n",
    "            r_list.append(np.squeeze(r))\n",
    "            break\n",
    "            \n",
    "        if copy:\n",
    "            q(memory.sample(1)[0])\n",
    "            q_target(memory.sample(1)[0])\n",
    "            q_target.set_weights(q.get_weights())\n",
    "            copy = False\n",
    "                        \n",
    "    # transition 몇 개 이상 모이면 학습\n",
    "    # 성공 경험이 있을 때 학습\n",
    "    if memory.size() > BATCH_SIZE and np.sum(r_list) > 0:\n",
    "        train(q, q_target, memory)\n",
    "        \n",
    "    if e != 0 and e%20 == 0:\n",
    "        \n",
    "        # 가끔 behaviour 를 target에 복사해줌\n",
    "        q_target.set_weights(q.get_weights())\n",
    "        \n",
    "    ipd.clear_output(wait=True)\n",
    "    plt.figure(facecolor='w',figsize=(10,1))\n",
    "    plt.plot(r_list)\n",
    "    plt.title(f\"{e+1}/{EPISODES}, {np.sum(r_list)/(e+1)*100:.4f}%, e={epsilon:.2f}\")\n",
    "    plt.show()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
