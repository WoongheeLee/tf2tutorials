{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning 에서 기본적으로 알아야할 정의, 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* David Silver 교수 슬라이드에 기반함 (https://www.davidsilver.uk/teaching/)\n",
    "* David Silver 교수 강의를 우리말로 잘 풀어서 설명해준 유투브 (https://www.youtube.com/watch?v=wYgyiCEkwC8&list=PLpRS2w0xWHTcTZyyX8LMmtbcMXpd3s4TU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reward, action, states, agents, environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information State\n",
    "* Markov state라고도 함\n",
    "\n",
    "**정의**\n",
    "상태(state) $S_t$는 마코프임 $\\Longleftrightarrow$\n",
    "$$\n",
    "\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, \\cdots, S_t]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "* Agent's의 정책(policy)\n",
    "* 키가 state, 값이 action인 매핑\n",
    "* Deterministic policy: $a=\\pi(s)$\n",
    "* Stochastic policy: $\\pi(a|s) = \\mathbb{P}[A_t = a | S_t = s$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "* 향후 얻을 reward 를 예측하는 가치 함수 (value function)\n",
    "* States의 좋음/나쁨 평가에 사용\n",
    "* Action 중에 뭐 고를꺼냐 할 때 사용할 수 있음\n",
    "$$\n",
    "v_\\pi (s) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* P: next state 예측. 어떤 state(s)에서 action(a)을 취할 때 어느 다음 state(s') 어디로 갈거냐 하는 확률. Transition matrix로 나타낼 수 있음\n",
    "* R: 다음 단계 (즉시 얻을) reward. 어떤 state(s)에서 action(a)를 취할 때, 얻을 reward의 기대값\n",
    "$$\n",
    "P^a_{ss'} = \\mathbb{P}[S_{t+1}=s'|S_t = s, A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and Exploitation\n",
    "* exploration: 새로운 행동 해봄\n",
    "* exploitation: 에이전트가 알기에 리워드 좋을 행동만 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property\n",
    "**정의**\n",
    "* $S_t$ 는 마코프임 $\\Longleftrightarrow$\n",
    "$$\n",
    "\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, \\cdots, S_t]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Transition Matrix\n",
    "Markov state s 와 그 다음 뒤 따르는 state s'에 대해, *state transition probability* 는 다음과 같이 정의\n",
    "$$\n",
    "P_{ss'} = \\mathbb{P}[S_{t+1}=s'|S_t = s]\n",
    "$$\n",
    "\n",
    "State transition matrix $P$ 는 모든 s와 s' 조합으로 만듦\n",
    "$$\n",
    "P =\n",
    "\\begin{bmatrix}\n",
    "P_{11} \\cdots P_{1n}\\\\\n",
    "\\vdots\\\\\n",
    "P_{n1} \\cdots P_{nn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "이 때 각 row의 합은 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Process\n",
    "**정의**\n",
    "* Markov process (또는 Markov chain) 은 튜플 $<S, P>$ 임\n",
    "  * $S$: states의 (유한) 집합\n",
    "  * $P$: state transition probability matrix $P_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t =s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "마코프 프로세스에 reward 추가된거\n",
    "\n",
    "**정의**\n",
    "* Markov Reward Process 는 튜플 $<S, P, R, \\gamma>$\n",
    "  * $S$: states 의 유한 집합\n",
    "  * $P$: state transition probability matrix $P_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t =s]$\n",
    "  * $R$: reward function $R_s = \\mathbb{E}[R_{t+1} | S_t = s]$\n",
    "  * $\\gamma$: 할인 정도(discount factor) $gamma\\in[0, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return\n",
    "**정의**\n",
    "* 시간 t동안 디스카운트 된 리워드 총합을 리턴 $G_t$라고 함\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "**정의**\n",
    "* MRP의 state value function $v(s)$ 는 state s 에서 시작해서 얻을 return 기대값\n",
    "$$\n",
    "v(s) = \\mathbb{E} [G_t | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation for MRPs\n",
    "value function 은 아래 두 가지로 분리 가능\n",
    "* 즉시 얻을 reward $R_{t+1}$\n",
    "* 후속 상태의 discount 된 value $\\gamma v(S_{t+1})$\n",
    "$$\n",
    "\\begin{align}\n",
    "v(s) & = \\mathbb{E}[G_t | S_t=s]\\\\\n",
    "& = \\mathbb{E} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s]\\\\\n",
    "& = \\mathbb{E} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) | S_t = s]\\\\\n",
    "& = \\mathbb{E} [R_{t+1} + \\gamma G_{t+1} | S_t = s]\\\\\n",
    "& = \\mathbb{E} [R_{t+1} + \\gamma v(S_{t+1}) | S_t = s]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation in Matrix Form\n",
    "$$\n",
    "v = R + \\gamma P v\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "v(1)\\\\\n",
    "\\vdots\\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "R_1\\\\\n",
    "\\vdots\\\\\n",
    "R_n\n",
    "\\end{bmatrix}\n",
    "+\\gamma\n",
    "\\begin{bmatrix}\n",
    "P_{11} \\cdots P_{1n}\\\\\n",
    "\\vdots\\\\\n",
    "P_{n1} \\cdots P_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v(1)\\\\\n",
    "\\vdots\\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Bellman Equation\n",
    "* Bellman equation 이 linear equation 이므로\n",
    "$$\n",
    "\\begin{align}\n",
    "v & = R + \\gamma P v\\\\\n",
    "(I - \\gamma P) v & = R\\\\\n",
    "v & = (I-\\gamma P )^{-1} R\n",
    "\\end{align}\n",
    "$$\n",
    "* $n$개 states에 대해 $O(n^3)$ 의 계산량\n",
    "* Iterative methods 도 있음\n",
    "  * Dynamic programming\n",
    "  * Monte-Carlo evaluation\n",
    "  * Temporal-Difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov decision process (MDP)\n",
    "MRP에서 action이 추가 됨\n",
    "\n",
    "**정의**\n",
    "* MDP 는 $<S, A, P, R, \\gamma>$ 튜플임\n",
    "  * $S$: states 의 유한 집합\n",
    "  * $A$: action 의 유한 집합\n",
    "  * $P$: state transition probability matrix $P^a_{ss'} = \\mathbb{P} [S_{t+1} = s'|S_t = s, A_t=a]$\n",
    "  * $R$: reward function $R_s^a = \\mathbb{E}[R_{t+1} | S_t = s, A_t=a]$\n",
    "  * $\\gamma$: 할인 정도(discount factor) $gamma\\in[0, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "**정의**\n",
    "* 정책(policy) $\\pi$ 는 states가 주어졌을 때 action에 관한 분포\n",
    "$$\n",
    "\\pi(a|s) = \\mathbb{P} [A_t = a | S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MDP $M = <S, A, P, R, \\gamma>$ 와 policy $\\pi$가 주어졌을 때\n",
    "* state 시퀀스 $S_1, S_2, \\cdots$ 는 Markov process $<S, P^\\pi>$ 임\n",
    "* state와 reward 시퀀스 $S_1, R_2, S_2, \\cdots$ 는 MRP $<S, P^\\pi, R^\\pi, \\gamma> $임 \n",
    "* 이 때\n",
    "$$\n",
    "\\begin{align}\n",
    "P^\\pi_{s,s'} & = \\sum_{a\\in A} \\pi(a|s) P^a_{ss'}\\\\\n",
    "R^\\pi_s & = \\sum_{a\\in A} \\pi (a|s) R^a_s\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "**정의**\n",
    "* MDP의 state-value function $v_\\pi (s)$ 은 policy $\\pi$에 따라서 state s로부터 얻을 수 있는 return 기대값\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi [G_t | S_t = s]\n",
    "$$\n",
    "\n",
    "**정의**\n",
    "* action-value function $q_\\pi (s, a)$ 는 policy $\\pi$를 따라서, state s에서 action a를 취했을 때 얻을 return 기대값\n",
    "$$\n",
    "q_\\pi (s, a) = \\mathbb{E}_pi [G_t | S_t = s, A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equation\n",
    "위에서 정의한 리턴을 즉시 얻을 reward와 디스카운트 된 그 다음 state 의 value 나 action-value로 나타낼 수 있음\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi (S_{t+1}) | S_t = s\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_\\pi (s, a) = \\mathbb{E} [ R_{t+1} + \\gamma q_\\pi (S_{t+1}, A_{t+1} | S_t = s, A_t = a ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix 로 나타내면\n",
    "\n",
    "$$\n",
    "v_\\pi = R^\\pi + \\gamma P^\\pi v_\\pi\n",
    "$$\n",
    "\n",
    "그러면 해는\n",
    "\n",
    "$$\n",
    "v_\\pi = (I-\\gamma P^\\pi)^{-1} R^\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Value Function\n",
    "**정의**\n",
    "* optimal state-value function $v_* (s)$ 는 모든 policies 중에서 maximum value 임\n",
    "$$\n",
    "v_* (s) = \\max_\\pi v_\\pi (s)\n",
    "$$\n",
    "\n",
    "* optimal action-value function $q_* (s, a)$ 는 모든 policies 중에서 maximum action-value\n",
    "$$\n",
    "q_* (s, a) = \\max_\\pi q_\\pi (s, a)\n",
    "$$\n",
    "\n",
    "optimal value function 을 알면 MDP가 풀렸다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "\n",
    "**정리**\n",
    "\n",
    "어떤 MDP 에 대해서도\n",
    "* optimal policy $\\pi_*$가 있다. 이 policy는 다른 모든 policies 보다 더 좋거나 동일 $\\pi_* \\ge \\pi, \\forall \\pi$\n",
    "* 모든 optimal policies 는 optial value function을 얻을 수 잇음 $v_{\\pi_*}(s) = v_* (s)$\n",
    "* 모든 optial policies 는 optimal action-value function을 얻을 수 있음 $q_{\\pi_*} (s, a) = q_*(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an Optimal Policy\n",
    "* optimal action-value function 알면 optimal policy 구해짐\n",
    "* MDP 에서 policity 가 deterministic하게 됨(걍 action이 정해지니깐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Bellman Optimality Equation\n",
    "* Bellman Optimality Equation 은 비선형\n",
    "* 한 번에 못품\n",
    "* iterative methods\n",
    "  * Value Iteration\n",
    "  * Policy Iteration\n",
    "  * Q-learning\n",
    "  * Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "* Problem: 주어진 policy $\\pi$를 평가\n",
    "* Solution: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
