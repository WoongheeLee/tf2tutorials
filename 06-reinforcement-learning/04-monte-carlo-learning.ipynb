{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dynamic programming (policy iteration 과 value iteration) 에서는\n",
    "* MDP (transition matrix, reward) 를 알고 있을 때 문제 풀이\n",
    "* Monte-Carlo learning 과 TD learning (temporal-difference learning) 에서는 MDP 를 모를 때 value function 찾는 **Model-free prediction** 문제 푸는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motnte-Carlo Reinforcement Learning\n",
    "* MC 방법은 episodes 경험으로부터 풀이 하는 것\n",
    "* model-free: transition / rewards 모를 때 풀이\n",
    "  * 앞서 dynamic programming 은 transition, rewards 의 look up table (구현은 다차원 array) 을 가지고 함\n",
    "* MC 는 episodes 가 다 끝났을 때의 경험을 가지고 문제 풀이함 (no bootstrapping)\n",
    "* MC 의 기본 아이디어: value = mean return\n",
    "* 주의: MC는 episodic MDPs 에만 적용 가능\n",
    "  * 즉 모든 episodes가 완료(terminate)되어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Policy Evaluation\n",
    "* Goal: policy $\\pi$를 가지고 여러 에피소드(하나의 에피소드여도 되긴 함) 경험으로 부터 $v_\\pi$를 학습\n",
    "* return 은 discounted reward의 총합이라는 점을 다시 생각해보기\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-1} R_T\n",
    "$$\n",
    "* value function 은 return 기대값이라는 점을 다시 생각해보기\n",
    "$$\n",
    "v_\\pi (s) = \\mathbb{E}_\\pi [G_t | S_t = s]\n",
    "$$\n",
    "* Monte-Carlo policy evaluation 이 하는 일은 기대값을 뱉어주는게 아니라 에피소드들에서 얻은 경험적인 값의 평균을 뱉어주는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-Visit vs. Every-Visit\n",
    "* value function 의 값을 채울 때, state 마다 return 값을 채워줌\n",
    "* 평균 내려면 state 별 방문 횟수로 나누면 됨\n",
    "* 이 때, 한 episode 에 대해\n",
    "  * 여러 번 반복해서 방문한 state도 1회 방문으로 치면 first-visit\n",
    "  * 반복해서 방문한 횟수만큼으로 나누기 해주면 every-visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Monte-Carlo Updates\n",
    "* 시퀀스 $x_1, x_2, \\cdots$ 의 평균 $\\mu_1, \\mu_2, \\cdots$ 를 incrementally 구할 수 있음\n",
    "\\begin{align}\n",
    "\\mu_k & = {1\\over k} \\sum_{j=1}^k x_j\\\\\n",
    "& = {1 \\over k} \\big( x_k + \\sum_{j=1}^{k-1} x_j \\big)\\\\\n",
    "& = {1 \\over k} (x_k + (k-1) \\mu_{k-1})\\\\\n",
    "& = \\mu_{k-1} + {1\\over k} (x_k - \\mu_{k-1})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위와 같이 episodes $S_1, A_1, R_2, \\cdots , S_t$ 에 대해서 value function $V(s)$ 도 incrementally update할 수 있음\n",
    "* 각각 리턴 $G_t$ 를 가지는 state $S_t$ 에 대해서\n",
    "$$\n",
    "N(S_t) \\leftarrow N(S_t) + 1\n",
    "$$\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + {1\\over N(S_t)} (G_t - V(S_t))\n",
    "$$\n",
    "* 위 방법은 에피소드가 진행됨에 따라 $N(S_t)$ 가 점점 커져서, 새로운 경험이 적게 반영됨\n",
    "* Non-stationary (MDP 가 시간 지남에 따라 변화) 문제에서는 아래 방법으로 update가 나을 수 있음 (오래된 에피소드는 잊어주는 방식)\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
