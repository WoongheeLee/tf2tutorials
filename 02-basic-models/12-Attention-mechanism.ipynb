{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset define\n",
    "SOS_token = 0\n",
    "EOS_token = 11\n",
    "n_samples=6000\n",
    "seq_len = 5\n",
    "\n",
    "X = np.array([[np.random.randint(1, 6) for _ in range(seq_len)] for _ in range(n_samples)])\n",
    "Y = X+5\n",
    "\n",
    "X= np.insert(X, 0, SOS_token, axis=1)\n",
    "Y = np.insert(Y, seq_len, EOS_token, axis=1)\n",
    "\n",
    "x_data = X[:int(n_samples * 0.5),:]\n",
    "y_data = Y[:int(n_samples * 0.5),:]\n",
    "\n",
    "x_eval = X[int(n_samples * 0.5): ,:]\n",
    "y_eval = Y[int(n_samples * 0.5): ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----vocab----#\n",
    "vocab_size = 12\n",
    "embedding_dim =16\n",
    "\n",
    "#----training----#\n",
    "batch_size = 3000\n",
    "epochs = 1000\n",
    "\n",
    "#----encoder,decoder----#\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_data, y_data, batch_size):\n",
    "    n_samples = len(x_data)\n",
    "    while True:\n",
    "        batches = range(0, n_samples, batch_size)\n",
    "        for start in batches:\n",
    "            end = start + batch_size\n",
    "            X_batch = x_data[start:end]\n",
    "            Y_batch = y_data[start:end]\n",
    "            \n",
    "            all_data = {\n",
    "                'Encoder_input' : X_batch,\n",
    "                'Decoder_input' : Y_batch\n",
    "            }\n",
    "            yield (all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # make embedding matrix\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        '''\n",
    "        LSTM args.  \n",
    "            \n",
    "            return_sequences\n",
    "                True : [batch_size, input_seq_len, hidden_dim]  => return all sequence\n",
    "                False: [batch_size, hidden_dim] => return last output\n",
    "        '''\n",
    "        self.lstm = tf.keras.layers.LSTM(hidden_dim,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True) \n",
    "   \n",
    "    def call(self, inputs):\n",
    "        # embeded_input : [batch_size, input_seq_len, embedding_dim]\n",
    "        embeded_inputs = self.embedding(inputs)\n",
    "        output, memory_state, carry_state = self.lstm(embeded_inputs, initial_state = self.init_hidden_state())\n",
    "        return output, memory_state, carry_state\n",
    "    \n",
    "    def init_hidden_state(self):\n",
    "        return (tf.zeros((self.batch_size, self.hidden_dim)),tf.zeros((self.batch_size, self.hidden_dim))) #init state : tuple\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "        Attention Mechanism\n",
    "        1. encoder hidden state 생성\n",
    "        2. previous decoder hidden state와 모든 encoder hidden state와의 Alignment score 계산 => attention score\n",
    "        3. attention score softmax함 => attention weight\n",
    "        4. attention weight X 모든 encoder hidden state => context vector\n",
    "        5. previous decoder output과 context vector concat하여 Decoder의 input으로 사용함\n",
    "        6. 1~5반복\n",
    "    '''\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(hidden_dim)\n",
    "        self.W2 = tf.keras.layers.Dense(hidden_dim)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    def call(self, encoder_output, decoder_state, BahdanauAttn=True):\n",
    "        '''\n",
    "            encoder_output : [batch_size, time_step, hidden_dim]\n",
    "            decoder_state : [batch_size, hidden_dim]\n",
    "        '''\n",
    "        # encoder_output과 shape 맞춤\n",
    "        decoder_state = tf.expand_dims(decoder_state, 1)\n",
    "        if BahdanauAttn:\n",
    "            # self.W2(encoder_output)의 각 time step에 self.W1(decoder_state)더함\n",
    "            # [batch_size, time_step, 1]\n",
    "            attn_score = self.V(tf.nn.tanh(self.W1(decoder_state) + self.W2(encoder_output)))\n",
    "        else:\n",
    "            # dot_product\n",
    "            decoder_state = tf.transpose(decoder_state, perm=[0,2,1])\n",
    "            attn_score = tf.matmul(decoder_state,encoder_output)\n",
    "            \n",
    "        # [batch_size, input_seq_len, 1]\n",
    "        attn_weights = tf.nn.softmax(attn_score, axis =1)\n",
    "        \n",
    "        # [batch_size, input_seq_len, 1], 각 encoder_hidden에 attn_weigts 곱함\n",
    "        context_vector = attn_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = tf.keras.layers.LSTM(self.hidden_dim,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True)\n",
    "        self.attn = Attention(hidden_dim)\n",
    "        #vocab_logit\n",
    "        self.projection_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, inputs, prev_decoder_state, encoder_output):\n",
    "        context_vector,_ = self.attn(encoder_output, prev_decoder_state)\n",
    "\n",
    "        #embeded_inputs: [batch_size, 1 , embedding_dim]\n",
    "        embeded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        #new_input: [batch_size, 1 , embedding_dim + hidden_dim]\n",
    "        new_input = tf.concat([tf.expand_dims(context_vector, 1), embeded_inputs], axis=-1)\n",
    "        decoder_output, state, carry_state = self.lstm(new_input)\n",
    "\n",
    "        #[batch, hidden_dim]\n",
    "        decoder_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
    "        v =  self.projection_layer(decoder_output)\n",
    "        return v, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder, decoder, optimizer, loss_object, encoder_input, target):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_output, memory_state, carry_state= encoder(encoder_input)\n",
    "        # decoder init state : feature vector\n",
    "        hidden = memory_state \n",
    "        \n",
    "        # [[0]* batch_size]\n",
    "        decoder_input = tf.expand_dims([SOS_token] * batch_size, 1)\n",
    "        \n",
    "        # Teacher forcing \n",
    "        for t in range(0,target.shape[1]):\n",
    "            pred, hidden = decoder(decoder_input, hidden, encoder_output)\n",
    "            # pred와 target[:,t]가 매칭되도록 학습 \n",
    "            loss += loss_function(loss_object, target[:,t], pred)\n",
    "            decoder_input = tf.expand_dims(target[:,t], 1)\n",
    "            \n",
    "        batch_loss = (loss/int(decoder_input.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "        return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_run(epoch, encoder, decoder):  \n",
    "    generator= batch_generator(x_data,y_data,batch_size)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        total_loss = 0\n",
    "        for step in range(len(x_data)//batch_size):\n",
    "            data = next(generator)\n",
    "            batch_loss = train_step(encoder, decoder, optimizer, loss_object ,data['Encoder_input'] ,data['Decoder_input'])\n",
    "        if e % 50 ==0:\n",
    "            print(f'Epochs :{e}/{epoch}, \\t Batch_loss : {batch_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(loss_object, real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # padding된 부분 masking하여 loss에 영향을 주지 않도록함\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, n_samples= 10):\n",
    "    generator= batch_generator(x_data,y_data,1)\n",
    "    encoder.batch_size = 1\n",
    "    for i in range(n_samples):\n",
    "        data = next(generator)\n",
    "        inputs = data['Encoder_input']\n",
    "        target = data['Decoder_input'][:,:-1].squeeze()\n",
    "        encoder_output, memory_state, carry_state= encoder(inputs)\n",
    "        hidden = memory_state\n",
    "        \n",
    "        decoder_input = tf.expand_dims([0], 1)\n",
    "        result=''\n",
    "        for t in range(inputs.shape[1]):\n",
    "            pred, hidden = decoder(decoder_input, hidden, encoder_output)\n",
    "            pred_id = tf.argmax(pred,axis=1).numpy()\n",
    "            if pred_id ==EOS_token:\n",
    "                break\n",
    "            else :\n",
    "                result += str(pred_id[0]) +' '\n",
    "            decoder_input = tf.expand_dims(pred_id,1)\n",
    "        print(f'real: {target} \\t pred : {result}')\n",
    "        result=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs :0/1000, \t Batch_loss : 14.90906\n",
      "Epochs :50/1000, \t Batch_loss : 10.73083\n",
      "Epochs :100/1000, \t Batch_loss : 10.53481\n",
      "Epochs :150/1000, \t Batch_loss : 8.99809\n",
      "Epochs :200/1000, \t Batch_loss : 8.46741\n",
      "Epochs :250/1000, \t Batch_loss : 8.19937\n",
      "Epochs :300/1000, \t Batch_loss : 8.05801\n",
      "Epochs :350/1000, \t Batch_loss : 7.82328\n",
      "Epochs :400/1000, \t Batch_loss : 7.51935\n",
      "Epochs :450/1000, \t Batch_loss : 11.33467\n",
      "Epochs :500/1000, \t Batch_loss : 10.04626\n",
      "Epochs :550/1000, \t Batch_loss : 9.52231\n",
      "Epochs :600/1000, \t Batch_loss : 9.21502\n",
      "Epochs :650/1000, \t Batch_loss : 8.49149\n",
      "Epochs :700/1000, \t Batch_loss : 3.08244\n",
      "Epochs :750/1000, \t Batch_loss : 1.26239\n",
      "Epochs :800/1000, \t Batch_loss : 0.69617\n",
      "Epochs :850/1000, \t Batch_loss : 1.23023\n",
      "Epochs :900/1000, \t Batch_loss : 0.41134\n",
      "Epochs :950/1000, \t Batch_loss : 0.24137\n",
      "real: [8 8 8 7 7] \t pred : 8 8 8 7 7 \n",
      "real: [ 9  6 10  6  9] \t pred : 9 6 10 6 9 \n",
      "real: [7 7 6 9 6] \t pred : 7 7 6 9 6 \n",
      "real: [10  8  6  9  7] \t pred : 10 8 6 9 7 \n",
      "real: [6 7 9 8 9] \t pred : 6 7 9 8 9 \n",
      "real: [ 6  9  6 10 10] \t pred : 6 9 6 10 10 \n",
      "real: [6 7 8 9 9] \t pred : 6 7 8 9 9 \n",
      "real: [7 8 8 6 6] \t pred : 7 8 8 6 6 \n",
      "real: [ 8 10  8  7  7] \t pred : 8 10 8 7 7 \n",
      "real: [ 7  6 10  7  7] \t pred : 7 6 10 7 7 \n"
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size)\n",
    "    decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size)\n",
    "    train_run(epochs, encoder, decoder)\n",
    "    evaluate(encoder, decoder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
