{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./resources/Attention1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanism을 간단하게 말하자면, input의 중요한 부분에 집중하여 output 예측 정확도를 향상시는 기술입니다.\n",
    "\n",
    "아래의 예제 코드는 Encoder-Decoder 구조의 Attention mechanism입니다. 일반적으로 encoder에서 input을 feature vector로 압축하면, 저차원의 vector로 표현되기 때문에 input에 대한 모든 정보를 담기는 어렵습니다. 즉, 정보의 손실이 발생합니다. 따라서 decoder은 더 많은 정보를 알기 위해서, encoder의 모든 hidden state를 참고합니다. 참고를 할 때는 중요한 hidden state에 더 높은 가중치를 부여하고, decoder의 input에 값을 더하거나 concat하여 output 예측에 이용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset define\n",
    "SOS_token = 0\n",
    "EOS_token = 11\n",
    "n_samples=6000\n",
    "seq_len = 5\n",
    "\n",
    "X = np.array([[np.random.randint(1, 6) for _ in range(seq_len)] for _ in range(n_samples)])\n",
    "Y = X+5\n",
    "\n",
    "X= np.insert(X, 0, SOS_token, axis=1)\n",
    "Y = np.insert(Y, seq_len, EOS_token, axis=1)\n",
    "\n",
    "x_data = X[:int(n_samples * 0.5),:]\n",
    "y_data = Y[:int(n_samples * 0.5),:]\n",
    "\n",
    "x_eval = X[int(n_samples * 0.5): ,:]\n",
    "y_eval = Y[int(n_samples * 0.5): ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----vocab----#\n",
    "vocab_size = 12\n",
    "embedding_dim =32\n",
    "\n",
    "#----training----#\n",
    "batch_size = 3000\n",
    "epochs = 1000\n",
    "\n",
    "#----encoder,decoder----#\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_data, y_data, batch_size):\n",
    "    n_samples = len(x_data)\n",
    "    while True:\n",
    "        batches = range(0, n_samples, batch_size)\n",
    "        for start in batches:\n",
    "            end = start + batch_size\n",
    "            X_batch = x_data[start:end]\n",
    "            Y_batch = y_data[start:end]\n",
    "            \n",
    "            all_data = {\n",
    "                'Encoder_input' : X_batch,\n",
    "                'Decoder_input' : Y_batch\n",
    "            }\n",
    "            yield (all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # make embedding matrix\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        '''\n",
    "        LSTM args.  \n",
    "            \n",
    "            return_sequences\n",
    "                True : [batch_size, input_seq_len, hidden_dim]  => return all sequence\n",
    "                False: [batch_size, hidden_dim] => return last output\n",
    "        '''\n",
    "        self.lstm = tf.keras.layers.LSTM(hidden_dim,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True) \n",
    "   \n",
    "    def call(self, inputs):\n",
    "        # embeded_input : [batch_size, input_seq_len, embedding_dim]\n",
    "        embeded_inputs = self.embedding(inputs)\n",
    "        output, memory_state, carry_state = self.lstm(embeded_inputs, initial_state = self.init_hidden_state())\n",
    "        return output, memory_state, carry_state\n",
    "    \n",
    "    def init_hidden_state(self):\n",
    "        return (tf.zeros((self.batch_size, self.hidden_dim)),tf.zeros((self.batch_size, self.hidden_dim))) #init state : tuple\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./resources/Attention2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "        Attention Mechanism\n",
    "        1. encoder hidden state 생성\n",
    "        2. previous decoder hidden state와 모든 encoder hidden state와의 Alignment score 계산 => attention score\n",
    "        3. attention score softmax함 => attention weight\n",
    "        4. attention weight X 모든 encoder hidden state => context vector\n",
    "        5. previous decoder output과 context vector concat하여 Decoder의 input으로 사용함\n",
    "        6. 1~5반복\n",
    "    '''\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(hidden_dim)\n",
    "        self.W2 = tf.keras.layers.Dense(hidden_dim)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    def call(self, encoder_output, decoder_state, BahdanauAttn=True):\n",
    "        '''\n",
    "            encoder_output : [batch_size, time_step, hidden_dim]\n",
    "            decoder_state : [batch_size, hidden_dim]\n",
    "        '''\n",
    "        # encoder_output과 shape 맞춤\n",
    "        decoder_state = tf.expand_dims(decoder_state, 1)\n",
    "        \n",
    "        # Equation (4) [batch_size, time_step, 1]\n",
    "        # self.W2(encoder_output)의 각 time step에 self.W1(decoder_state)더함\n",
    "        attn_score = self.V(tf.nn.tanh(self.W1(decoder_state) + self.W2(encoder_output)))\n",
    "    \n",
    "        # Equation (1) [batch_size, input_seq_len, 1]\n",
    "        attn_weights = tf.nn.softmax(attn_score, axis =1)\n",
    "        \n",
    "        # Equation (2)\n",
    "        # [batch_size, input_seq_len, 1], 각 encoder_hidden에 attn_weigts 곱함\n",
    "        context_vector = attn_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = tf.keras.layers.LSTM(self.hidden_dim,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True)\n",
    "        self.attn = Attention(hidden_dim)\n",
    "        #vocab_logit\n",
    "        self.projection_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, inputs, prev_decoder_state, encoder_output):\n",
    "        context_vector,_ = self.attn(encoder_output, prev_decoder_state)\n",
    "\n",
    "        #embeded_inputs: [batch_size, 1 , embedding_dim]\n",
    "        embeded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        # Equation (3)\n",
    "        #new_input: [batch_size, 1 , embedding_dim + hidden_dim]\n",
    "        new_input = tf.concat([tf.expand_dims(context_vector, 1), embeded_inputs], axis=-1)\n",
    "        decoder_output, state, carry_state = self.lstm(new_input)\n",
    "\n",
    "        #[batch, hidden_dim]\n",
    "        decoder_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
    "        v =  self.projection_layer(decoder_output)\n",
    "        return v, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder, decoder, optimizer, loss_object, encoder_input, target):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_output, memory_state, carry_state= encoder(encoder_input)\n",
    "        # decoder init state : feature vector\n",
    "        hidden = memory_state \n",
    "        \n",
    "        # [[0]* batch_size]\n",
    "        decoder_input = tf.expand_dims([SOS_token] * batch_size, 1)\n",
    "        \n",
    "        # Teacher forcing \n",
    "        for t in range(0,target.shape[1]):\n",
    "            pred, hidden = decoder(decoder_input, hidden, encoder_output)\n",
    "            # pred와 target[:,t]가 매칭되도록 학습 \n",
    "            loss += loss_function(loss_object, target[:,t], pred)\n",
    "            decoder_input = tf.expand_dims(target[:,t], 1)\n",
    "            \n",
    "        batch_loss = (loss/int(decoder_input.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "        return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_run(epoch, encoder, decoder, x_data, y_data):  \n",
    "    generator= batch_generator(x_data,y_data,batch_size)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        total_loss = 0\n",
    "        for step in range(len(x_data)//batch_size):\n",
    "            data = next(generator)\n",
    "            batch_loss = train_step(encoder, decoder, optimizer, loss_object ,data['Encoder_input'] ,data['Decoder_input'])\n",
    "        if e % 50 ==0:\n",
    "            print(f'Epochs :{e}/{epoch}, \\t Batch_loss : {batch_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(loss_object, real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # padding된 부분 masking하여 loss에 영향을 주지 않도록함\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, x_eval, y_eval, n_samples= 10):\n",
    "    generator= batch_generator(x_eval,y_eval,1)\n",
    "    encoder.batch_size = 1\n",
    "    for i in range(n_samples):\n",
    "        data = next(generator)\n",
    "        inputs = data['Encoder_input']\n",
    "        target = data['Decoder_input'][:,:-1].squeeze()\n",
    "        encoder_output, memory_state, carry_state= encoder(inputs)\n",
    "        hidden = memory_state\n",
    "        \n",
    "        decoder_input = tf.expand_dims([0], 1)\n",
    "        result=''\n",
    "        for t in range(inputs.shape[1]):\n",
    "            pred, hidden = decoder(decoder_input, hidden, encoder_output)\n",
    "            pred_id = tf.argmax(pred,axis=1).numpy()\n",
    "            if pred_id ==EOS_token:\n",
    "                break\n",
    "            else :\n",
    "                result += str(pred_id[0]) +' '\n",
    "            decoder_input = tf.expand_dims(pred_id,1)\n",
    "        print(f'real: {target} \\t pred : {result}')\n",
    "        result=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs :0/1000, \t Batch_loss : 14.91170\n",
      "Epochs :50/1000, \t Batch_loss : 10.73672\n",
      "Epochs :100/1000, \t Batch_loss : 10.49396\n",
      "Epochs :150/1000, \t Batch_loss : 8.62315\n",
      "Epochs :200/1000, \t Batch_loss : 6.33976\n",
      "Epochs :250/1000, \t Batch_loss : 3.31085\n",
      "Epochs :300/1000, \t Batch_loss : 1.09924\n",
      "Epochs :350/1000, \t Batch_loss : 0.43133\n",
      "Epochs :400/1000, \t Batch_loss : 0.20957\n",
      "Epochs :450/1000, \t Batch_loss : 0.12081\n",
      "Epochs :500/1000, \t Batch_loss : 0.07761\n",
      "Epochs :550/1000, \t Batch_loss : 0.05087\n",
      "Epochs :600/1000, \t Batch_loss : 0.03482\n",
      "Epochs :650/1000, \t Batch_loss : 0.02529\n",
      "Epochs :700/1000, \t Batch_loss : 0.01985\n",
      "Epochs :750/1000, \t Batch_loss : 0.01614\n",
      "Epochs :800/1000, \t Batch_loss : 0.01346\n",
      "Epochs :850/1000, \t Batch_loss : 0.01146\n",
      "Epochs :900/1000, \t Batch_loss : 0.00991\n",
      "Epochs :950/1000, \t Batch_loss : 0.00867\n",
      "real: [9 8 6 6 6] \t pred : 9 8 6 6 6 \n",
      "real: [ 6  9 10  8  7] \t pred : 6 9 10 8 7 \n",
      "real: [9 6 6 8 7] \t pred : 9 6 6 8 7 \n",
      "real: [ 6  9  7  7 10] \t pred : 6 9 7 7 10 \n",
      "real: [9 9 8 8 9] \t pred : 9 9 8 8 9 \n",
      "real: [8 6 9 6 8] \t pred : 8 6 9 6 8 \n",
      "real: [10 10  9  9  6] \t pred : 10 10 9 9 6 \n",
      "real: [ 6  8  9 10  7] \t pred : 6 8 9 10 7 \n",
      "real: [ 8  8 10  6  7] \t pred : 8 8 10 6 7 \n",
      "real: [ 9 10  9  9  9] \t pred : 9 10 9 9 9 \n"
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size)\n",
    "    decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size)\n",
    "    train_run(epochs, encoder, decoder, x_data, y_data)\n",
    "    evaluate(encoder, decoder, x_eval ,y_eval)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
