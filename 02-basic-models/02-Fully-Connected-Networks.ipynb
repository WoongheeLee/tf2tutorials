{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected network란, 뭐, 공식적인 이름은 아닌데요, fully connected layer로만 이루어진 네트워크를 말합니다. Activation function이 자유로운 multi-layer perceptron이라고 보시면 됩니다.\n",
    "\n",
    "![image](./resources/fully-connected-networks.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 필요한 모듈 임포트해보겠습니다. 편의를 위해서 이전 노트북(01-Tensorflow-Basic)에서 보셨던 데이터를 로드하고 전처리(standardization)하는 코드는 ``data_utils``라는 모듈에 모아놨습니다.\n",
    "이제, 그것을 단순히 호출한 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers\n",
    "\n",
    "from utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 여러분은 이전 노트북(01-Tensorflow-Basic)에서 이미 fully connected network를 보셨습니다. 다음 코드가 바로 그 코드인데요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저, 이미지를 벡터로 펼치고,\n",
    "# 128개의 hidden unit을 가지는 dense(fully connected) layer\n",
    "# 128개의 hidden unit을 가지는 dense(fully connected) layer\n",
    "# 10개의 hidden unit을 가지는 dense(fully connected) layer\n",
    "\n",
    "myfirstmodel = models.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=tf.nn.tanh),\n",
    "    layers.Dense(128, activation=tf.nn.tanh),\n",
    "    layers.Dense(10, activation=tf.nn.softmax),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드에 대해서 설명을 드려보면,\n",
    "\n",
    "- ``models.Sequential``: ``models``은 ``tensorflow.keras`` 모듈 아래에 존재하는 모듈로, 그 모듈 안에 ``Sequential``이라는 클래스가 존재합니다. ``Sequential``은 ``tensorflow.keras.models.Model``을 상속하며, sequential한 레이어를 이용하여 간단한 모델을 구축할 때 이용됩니다(이름 그대로 sequential 입니다). Sequential한 레이어를 이용한다는 것은, ``Sequential``에 데이터가 들어가면, ``Sequential``을 구성하고 있는 레이어를 차례차례, sequential하게 지나서 결과로 나오게 됩니다. 이 경우, 데이터가 들어가면, ``Flatten`` 레이어를 지나서 ``Dense`` 3개를 지나서 결과로 나오게 되는 것이죠. 이처럼 레이어를 sequential하게 쌓는 간단한 모델을 구현할 때 유용한 것이 ``Sequential`` 객체입니다.\n",
    "- ``layers.Flatten()``: 데이터를 flatten 시켜주는 레이어로, 데이터가 (BATCH_SIZE, H, W, C)처럼 고차원으로 들어왔을 경우, 배치 차원을 제외한 차원을 모두 flatten시켜줍니다. 즉, (BATCH_SIZE, H\\*W\\*C) 차원으로 결과가 나오게 됩니다. 하나의 샘플의 차원이 (H, W, C)에서 (H\\*W\\*C)인 벡터가 되버리는 것이죠.\n",
    "- ``layers.Dense(128, activation=tf.nn.tanh)``: fully connected layer입니다. 데이터를 입력으로 받아서, 128개의 뉴런의 아웃풋을 만들어내는 fully connected layer입니다. 이때, 마지막 결과를 내기 전에 ``activation=tf.nn.tanh``가 적용됩니다. ``activation=None``으로 두면 linear activation(activation이 없는 효과)의 효과를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
