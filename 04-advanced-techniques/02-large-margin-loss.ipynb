{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목적\n",
    "* 로스에 미분이 필요한 경우 어떻게 해야하나?\n",
    "* Large margin loss 를 구현해보기 ([논문은 링크에서 볼 수 있음](https://arxiv.org/abs/1803.05598))\n",
    "* 이 노트북은 커스텀화 된 로스 구현 외에도 텐서플로우2의 아래 메쏘드에 관해 배울 수 있음\n",
    "  * tf.GradientTape() 를 써서 함수를 미분하면 어떤 모양이 되나 확인\n",
    "  * tf.gather_nd 가 뭔 일을 하나 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers\n",
    "import numpy as np\n",
    "import time\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2 # 개 오래 걸려서 조금만 돌리자\n",
    "NB_CLASS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 걸 알아보기 전에 먼저 데이터랑 모델을 준비하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 28, 28, 1) (30000,)\n",
      "(10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = np.expand_dims(np.array(X_train, dtype=np.float32), axis=-1)\n",
    "X_test = np.expand_dims(np.array(X_test, dtype=np.float32), axis=-1)\n",
    "\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = np.random.randint(0, len(X_train), size=30000)\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(BATCH_SIZE)\n",
    "testdataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(models.Model):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.c1 = layers.Conv2D(256, (3,3), activation=tf.nn.relu)\n",
    "        self.c2 = layers.Conv2D(256, (3,3), activation=tf.nn.relu)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.latent = layers.Dense(4, activation=tf.nn.tanh)\n",
    "        self.dense = layers.Dense(10, activation=tf.nn.softmax)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.c1(inputs)\n",
    "        x = self.c2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.latent(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "classifier = Classifier()\n",
    "classifier.build((None, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. tf.GradientTape()가 미분하면 무슨 모양이 되나?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* classifier 에서는 BATCH_SIZE X 10 차원의 값이 출력 됨\n",
    "* 요걸 input x로 미분하면 무슨 모양? (이때 x = BATCH_SIZE X 28 X 28 X 1 차원임)\n",
    "* 상식적으로 BATCH_SIZE X 10 X 28 X 28 X 1 이 될 것 같지만, 미분 된 모양은 BATCH_SIZE X 28 X 28 X 1 이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (1, 28, 28, 1)\n",
      "y (1, 10)\n",
      "grads (1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((1, 28, 28, 1), dtype=tf.float32)\n",
    "print('x', x.shape)\n",
    "\n",
    "y_class = list()\n",
    "# persistent=True 를 해두면 이 tape로 몇 번이고 미분을 할 수 있다. 아니면 한 번 하면 끝\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = classifier(x)\n",
    "    print('y', y.shape)\n",
    "    \n",
    "    for i in range(NB_CLASS):\n",
    "        y_class.append(y[0][i])\n",
    "    \n",
    "grads = tape.gradient(y, x)\n",
    "print('grads',grads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그래서 어거지로 클래스별로 미분을 하려고 하면 위 코드처럼 with 블록 안에 클래스별로 분해된 값을 따로 저장해두어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "grad_list = list()\n",
    "\n",
    "for i in range(NB_CLASS):\n",
    "    grad_list.append(tape.gradient(y_class[i], x))\n",
    "    \n",
    "grad_list = np.array(grad_list)\n",
    "print(grad_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf.gather_nd\n",
    "* 인덱스 위치에 해당하는 텐서를 가져옴\n",
    "\n",
    "**주의**\n",
    "* 이름 비슷한 tf.gather가 있지만 이 놈은 텐서를 잘개 쪼개서 값을 가져오는게 아님\n",
    "* 이놈을 쓰려면 params에 넣을 텐서를 이런 저런 모양 변환을 해주어야 해서 복잡하니 그냥 tf.gather_nd나 쓰자. ([링크 참고](https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  6, 24],\n",
       "       [32, 71,  2],\n",
       "       [74, 49, 65],\n",
       "       [98, 66, 95]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.random.randint(0, 100, size=(4,3))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([32, 24,  6, 65, 95])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(params=p, indices=[[1,0],[0,2],[0,1],[2,2],[3,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Margin Loss\n",
    "* 1, 2 두 가지를 이용해서 large margin loss 를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = losses.SparseCategoricalCrossentropy()\n",
    "acc_obj = metrics.SparseCategoricalAccuracy()\n",
    "opt = optimizers.RMSprop(1e-4)\n",
    "\n",
    "loss = metrics.Mean()\n",
    "acc = metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_distance = losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래는 원 논문의 로스 수식을 그대로 구현 함\n",
    "\n",
    "$$\n",
    "\\hat{w} \\overset{\\triangle}{=} \\arg \\min_w \\sum_{\\mathscr{l}, k} \\mathscr{A}_{i\\neq y_k} \\max \\bigg\\{0, \\gamma_\\mathscr{l} + {f_i (x_k) - f_{y_k} (x_k) \\over \\epsilon + \\lVert \\nabla_{h_\\mathscr{l}} f_i (x_k) - \\nabla_{h_\\mathscr{l}} f_{y_k} (x_k)\\rVert_q } \\bigg\\}\n",
    "$$\n",
    "\n",
    "(원 저자들은 연산량을 줄이기 위해 정답 클래스와 가장 큰 차이나는 오답 클래스 가져다 그것만 썼지만 여기선 그런거 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_margin(xk, _y):\n",
    "    # 학습 시키다 보면 데이터 개수가 배치보다 작은 경우 있어서\n",
    "    _BATCH_SIZE = len(xk)\n",
    "    \n",
    "    # 정답 인덱싱 위한 것\n",
    "    yk = np.array(list(zip(np.arange(_BATCH_SIZE), _y))) # BATCH_SIZE X 2\n",
    "    \n",
    "    # f_i (x_k) : tape 로 미분한 모양이 우리 생각과 달라서 리스트 만들어둠. 위 1. tf.GradientTape()가 미분하면 무슨 모양이 되나?를 확인\n",
    "    _fi = list()\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(xk)\n",
    "        h0 = classifier.layers[0](xk) # conv2d\n",
    "        h1 = classifier.layers[1](h0) # conv2d_1\n",
    "        _x = classifier.layers[2](h1) # flatten\n",
    "        h2 = classifier.layers[3](_x) # dense\n",
    "        fi = classifier.layers[4](h2) # dense_1\n",
    "\n",
    "        for i in range(NB_CLASS):\n",
    "            _fi.append(fi[:,i]) # BATCH_SIZE X 1 (클래스마다)\n",
    "        # f_{y_k} (x_k),  정답 인덱싱으로부터 정답 소프트맥스 값만으로 된 f_yk (xk) 만듦\n",
    "        fyk = tf.gather_nd(fi, yk) # 위 2. tf.gather_nd 설명 보자\n",
    "        # \n",
    "        fyk = tf.transpose(tf.cast([fyk for _ in range(NB_CLASS)], dtype=tf.float32))\n",
    "        \n",
    "    # 분자\n",
    "    numerator = fi - fyk\n",
    "\n",
    "    # 분모\n",
    "    _loss = get_grads(tape, numerator, _fi, yk, fyk, xk)\n",
    "    _loss += get_grads(tape, numerator, _fi, yk, fyk, h0)\n",
    "    _loss += get_grads(tape, numerator, _fi, yk, fyk, h1)\n",
    "    _loss += get_grads(tape, numerator, _fi, yk, fyk, h2)\n",
    "            \n",
    "    return _loss\n",
    "\n",
    "def get_grads(tape, numerator, _fi, yk, fyk, hi):\n",
    "    # minimum distance gamma\n",
    "    _gamma = .5\n",
    "    _BATCH_SIZE = len(yk)\n",
    "    # 분모\n",
    "    Dfi = list()\n",
    "    for i in range(NB_CLASS):\n",
    "        # 기계적으로 계산하기 위해, 분자 0 안되도록 작은 수 더함\n",
    "        Dfi.append(tape.gradient(_fi[i], hi)+1e-15)\n",
    "    Dfi = tf.cast(Dfi, dtype=tf.float32)\n",
    "    Dfyk = tape.gradient(fyk, hi)\n",
    "\n",
    "    _loss = 0\n",
    "    for j in range(_BATCH_SIZE):\n",
    "        for i in range(NB_CLASS):\n",
    "            if yk[j][1] != i: # 정답 클래스 아닌 놈들에 대해서만 로스 계산해야함\n",
    "                _loss += (_gamma + tf.math.maximum(0, (numerator[j,i]) / l2_distance(Dfi[i,j], Dfyk[j]))) \n",
    "                \n",
    "    return _loss\n",
    "\n",
    "def train_step(inputs):\n",
    "    xk, _y = inputs\n",
    "    \n",
    "    with tf.GradientTape() as tape: \n",
    "        _loss = large_margin(xk, _y)\n",
    "        # acc 확인용\n",
    "        pred = classifier(xk)\n",
    "        \n",
    "    grads = tape.gradient(_loss, classifier.trainable_variables)\n",
    "    opt.apply_gradients(list(zip(grads, classifier.trainable_variables)))\n",
    "    \n",
    "    loss.update_state(_loss)\n",
    "    acc.update_state(acc_obj(_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(inputs):\n",
    "    _X, _y = inputs\n",
    "    \n",
    "    pred = classifier(_X)\n",
    "    _loss = loss_obj(_y, pred)\n",
    "        \n",
    "    loss.update_state(_loss)\n",
    "    acc.update_state(acc_obj(_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938, 2/2, loss=7750.85693359, acc=75.50%, 1.74secs/batch, 3580.28secs/epoch, 7162.37secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for e in range(EPOCHS):\n",
    "    epoch_time = time.time()\n",
    "    for i, x in enumerate(traindataset):\n",
    "        batch_time = time.time()\n",
    "        train_step(x)\n",
    "        ipd.clear_output(wait=True)\n",
    "        print(f\"{(i+1)}/{len(X_train)//BATCH_SIZE+1}, {e+1}/{EPOCHS}, loss={loss.result():.8f}, acc={100*acc.result():.2f}%, {time.time()-batch_time:.2f}secs/batch, {time.time()-epoch_time:.2f}secs/epoch, {time.time()-start_time:.2f}secs\")\n",
    "    loss.reset_states()\n",
    "    acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=78.12%\n"
     ]
    }
   ],
   "source": [
    "for x in testdataset:\n",
    "    test_step(x)\n",
    "    \n",
    "print(f\"acc={100*acc.result():.2f}%\")\n",
    "loss.reset_states()\n",
    "acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: cross entropy loss\n",
    "걍 단순한 로스 쓰면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "    xk, _y = inputs\n",
    "    \n",
    "    with tf.GradientTape() as tape: \n",
    "        pred = classifier(xk)\n",
    "        _loss = cce(_y, pred)\n",
    "        \n",
    "    grads = tape.gradient(_loss, classifier.trainable_variables)\n",
    "    opt.apply_gradients(list(zip(grads, classifier.trainable_variables)))\n",
    "    \n",
    "    loss.update_state(_loss)\n",
    "    acc.update_state(acc_obj(_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(inputs):\n",
    "    _X, _y = inputs\n",
    "    \n",
    "    pred = classifier(_X)\n",
    "    _loss = loss_obj(_y, pred)\n",
    "        \n",
    "    loss.update_state(_loss)\n",
    "    acc.update_state(acc_obj(_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938, 2/2, loss=1.29963875, acc=68.75%, 0.01secs/batch, 14.45secs/epoch, 28.91secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for e in range(EPOCHS):\n",
    "    epoch_time = time.time()\n",
    "    for i, x in enumerate(traindataset):\n",
    "        batch_time = time.time()\n",
    "        train_step(x)\n",
    "        ipd.clear_output(wait=True)\n",
    "        print(f\"{(i+1)}/{len(X_train)//BATCH_SIZE+1}, {e+1}/{EPOCHS}, loss={loss.result():.8f}, acc={100*acc.result():.2f}%, {time.time()-batch_time:.2f}secs/batch, {time.time()-epoch_time:.2f}secs/epoch, {time.time()-start_time:.2f}secs\")\n",
    "    loss.reset_states()\n",
    "    acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=70.02%\n"
     ]
    }
   ],
   "source": [
    "for x in testdataset:\n",
    "    test_step(x)\n",
    "    \n",
    "print(f\"acc={100*acc.result():.2f}%\")\n",
    "loss.reset_states()\n",
    "acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cce: 더 빠르고 구림\n",
    "* large margin loss: 느리고 좋음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
